sudo /opt/vitis_ai/conda/envs/vitis-ai-pytorch/bin/python fpganet-qat.py --device "cpu" --quant_mode calib --subset_len 1500 


# To deploy model and get the qunatized model in (onnx, pth, xmodel formta)
sudo /opt/vitis_ai/conda/envs/vitis-ai-pytorch/bin/python fgpgnet-qat.py --quant_mode test --subset_len 1 --batch_size=1  --deploy


vai_c_xir -x ./quantize_result/MyLargeModel_int.xmodel -a arch.json -o ./compiled -n b3_48_28_28.pth

sudo -E PATH=$PATH:/opt/vitis_ai/conda/envs/vitis-ai-pytorch/bin /opt/vitis_ai/conda/envs/vitis-ai-pytorch/bin/vai_c_xir -x ./quantize_result/MobileNetV3_int.xmodel -a arch.json -o ./compiled -n fgpgnet-L20



......
hardware aware quantization

sudo /opt/vitis_ai/conda/envs/vitis-ai-pytorch/bin/python fpganet-qat.py --quant_mode calib --target DPUCAHX8L_ISA0_SP --subset_len 200
