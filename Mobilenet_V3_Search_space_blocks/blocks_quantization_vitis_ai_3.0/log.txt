-----------------------Strating Qt ResidualBlock-in_136 14 14-out_136-k_3-e_4-s_1-act_h_swish-use_se_True.pth -----------------------------

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_136x14x14-out_136-k_3-e_4-s_1-act_h_swish-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization calibration process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/17 [00:00<?, ?it/s]                                                  | 0/17 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–‰                                               | 1/17 [00:00<00:00, 6168.09it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                            | 2/17 [00:00<00:00, 1936.43it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 3/17 [00:00<00:00, 2067.86it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                      | 4/17 [00:00<00:00, 2274.88it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 5/17 [00:00<00:00, 1693.16it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 6/17 [00:00<00:00, 1862.07it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 7/17 [00:00<00:00, 2074.33it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1205, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 8/17 [00:00<00:00, 2148.58it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 9/17 [00:00<00:00, 2258.37it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 10/17 [00:00<00:00, 2355.29it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 | 11/17 [00:00<00:00, 2440.61it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 12/17 [00:00<00:00, 2516.20it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 13/17 [00:00<00:00, 2417.68it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 14/17 [00:00<00:00, 2479.01it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.27, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 15/17 [00:00<00:00, 2547.77it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16/17 [00:00<00:00, 2549.83it/s, OpInfo: name = return_0, type = Return]      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 2657.00it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 13
  0%|          | 0/13 [00:00<?, ?it/s]/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/torchquantizer.py:223: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  bnfp[1] = stats.mode(data)[0][0]
  8%|â–Š         | 1/13 [00:00<00:07,  1.70it/s] 15%|â–ˆâ–Œ        | 2/13 [00:01<00:05,  1.85it/s] 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:01<00:05,  1.94it/s] 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:02<00:04,  1.96it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:02<00:04,  1.99it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:03<00:03,  1.91it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:03<00:03,  1.95it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:04<00:02,  1.97it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:04<00:02,  1.99it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:05<00:01,  2.00it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:05<00:00,  2.01it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:06<00:00,  2.01it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:06<00:00,  2.37it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:06<00:00,  2.05it/s]

[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quantized/quant_info.json)[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0
-------- End of ResidualBlock-in_136x14x14-out_136-k_3-e_4-s_1-act_h_swish-use_se_True.pth test 
Compilation

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_136x14x14-out_136-k_3-e_4-s_1-act_h_swish-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization test process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/17 [00:00<?, ?it/s]                                                  | 0/17 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–‰                                               | 1/17 [00:00<00:00, 9078.58it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                            | 2/17 [00:00<00:00, 2988.46it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 3/17 [00:00<00:00, 2968.37it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                      | 4/17 [00:00<00:00, 3147.10it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 5/17 [00:00<00:00, 2543.24it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 6/17 [00:00<00:00, 2718.87it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 7/17 [00:00<00:00, 2984.06it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1205, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 8/17 [00:00<00:00, 3031.11it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 9/17 [00:00<00:00, 3123.60it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 10/17 [00:00<00:00, 3222.42it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 | 11/17 [00:00<00:00, 3266.59it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 12/17 [00:00<00:00, 3329.91it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 13/17 [00:00<00:00, 3185.30it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 14/17 [00:00<00:00, 3206.13it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.27, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 15/17 [00:00<00:00, 3260.67it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16/17 [00:00<00:00, 3219.42it/s, OpInfo: name = return_0, type = Return]      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 3359.08it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 1
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 44.31it/s]

[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0

[0;32m[VAIQ_NOTE]: =>Dumping 'ResidualBlock'' checking data...[0m

[0;32m[VAIQ_NOTE]: =>Finish dumping data.(quantize_result/deploy_check_data_int/ResidualBlock)[0m

[0;32m[VAIQ_NOTE]: =>Successfully convert 'ResidualBlock' to xmodel.(quantize_result/ResidualBlock_int.xmodel)[0m
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/quantizerimpl.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if inf.sum() > 0 or nan.sum() > 0:
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/fix_ops.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (isinstance(tensor, torch.Tensor) and
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/adaptive_avg_pool.py:41: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  input_size = [int(dim) for dim in input.shape[2:]]

[0;32m[VAIQ_NOTE]: ResidualBlock_int.pt is generated.(quantize_result/ResidualBlock_int.pt)[0m
Xmodel deployed
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input ResidualBlock::input_0
  warnings.warn(

[0;32m[VAIQ_NOTE]: ResidualBlock_int.onnx is generated.(quantize_result/ResidualBlock_int.onnx)[0m
-------- End of ResidualBlock-in_136x14x14-out_136-k_3-e_4-s_1-act_h_swish-use_se_True.pth test 
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: null
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B2304_0101000016010405
[UNILOG][INFO] Graph name: ResidualBlock, with op num: 57
[UNILOG][INFO] Begin to compile...
module_infer = 0, counter_m[module_infer] = 48, counter_p[module_infer] = 48
module_idx = 0, counter_m[module_idx] = 48, counter_p[module_idx] = 48
module_idx = 1, counter_m[module_idx] = 0, counter_p[module_idx] = 0
module_idx = 2, counter_m[module_idx] = 46, counter_p[module_idx] = 46
module_idx = 3, counter_m[module_idx] = 2, counter_p[module_idx] = 2
[0;33m[UNILOG][WARNING] subgraph_ResidualBlock__ResidualBlock_MBConvLayer_conv__Sequential_depth_conv__Conv2d_conv__ret_9 switch to no prefetch mode because of PM failure: .
[m[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/ResidualBlock-in_136x14x14-out_136-k_3-e_4-s_1-act_h_swish-use_se_True.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 93544c890f4cdec68721a6deb1c8a6ef, and has been saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************



---------------------------Done Block 1-------------------------


-----------------------Strating Qt ResidualBlock-in_136 14 14-out_136-k_5-e_4-s_1-act_h_swish-use_se_True.pth -----------------------------

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_136x14x14-out_136-k_5-e_4-s_1-act_h_swish-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization calibration process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/17 [00:00<?, ?it/s]                                                  | 0/17 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–‰                                               | 1/17 [00:00<00:00, 9404.27it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                            | 2/17 [00:00<00:00, 3257.71it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 3/17 [00:00<00:00, 3170.30it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                      | 4/17 [00:00<00:00, 3253.29it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 5/17 [00:00<00:00, 2811.20it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 6/17 [00:00<00:00, 2965.57it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 7/17 [00:00<00:00, 3210.51it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1205, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 8/17 [00:00<00:00, 3258.66it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 9/17 [00:00<00:00, 3323.24it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 10/17 [00:00<00:00, 3399.23it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 | 11/17 [00:00<00:00, 3449.26it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 12/17 [00:00<00:00, 3486.78it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 13/17 [00:00<00:00, 3370.38it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 14/17 [00:00<00:00, 3402.10it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.27, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 15/17 [00:00<00:00, 3456.65it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16/17 [00:00<00:00, 3411.74it/s, OpInfo: name = return_0, type = Return]      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 3551.13it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 13
  0%|          | 0/13 [00:00<?, ?it/s]/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/torchquantizer.py:223: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  bnfp[1] = stats.mode(data)[0][0]
  8%|â–Š         | 1/13 [00:00<00:06,  1.90it/s] 15%|â–ˆâ–Œ        | 2/13 [00:01<00:05,  1.92it/s] 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:01<00:05,  1.94it/s] 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:02<00:04,  1.96it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:02<00:04,  2.00it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:03<00:03,  2.00it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:03<00:02,  2.01it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:04<00:02,  2.01it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:04<00:01,  2.01it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:05<00:01,  2.01it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:05<00:01,  2.00it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:06<00:00,  1.95it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:06<00:00,  2.31it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:06<00:00,  2.06it/s]

[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quantized/quant_info.json)[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0
-------- End of ResidualBlock-in_136x14x14-out_136-k_5-e_4-s_1-act_h_swish-use_se_True.pth test 
Compilation

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_136x14x14-out_136-k_5-e_4-s_1-act_h_swish-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization test process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/17 [00:00<?, ?it/s]                                                  | 0/17 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–‰                                               | 1/17 [00:00<00:00, 5447.15it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                            | 2/17 [00:00<00:00, 1930.63it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 3/17 [00:00<00:00, 1719.92it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                      | 4/17 [00:00<00:00, 1770.31it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 5/17 [00:00<00:00, 1778.45it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 6/17 [00:00<00:00, 1963.17it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 7/17 [00:00<00:00, 2190.24it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1205, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 8/17 [00:00<00:00, 2301.56it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 9/17 [00:00<00:00, 2414.84it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 10/17 [00:00<00:00, 2521.07it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 | 11/17 [00:00<00:00, 2569.32it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 12/17 [00:00<00:00, 2603.27it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 13/17 [00:00<00:00, 2562.19it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 14/17 [00:00<00:00, 2612.81it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.27, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 15/17 [00:00<00:00, 2684.41it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16/17 [00:00<00:00, 2673.45it/s, OpInfo: name = return_0, type = Return]      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 2785.39it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 1
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 28.60it/s]

[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0

[0;32m[VAIQ_NOTE]: =>Dumping 'ResidualBlock'' checking data...[0m

[0;32m[VAIQ_NOTE]: =>Finish dumping data.(quantize_result/deploy_check_data_int/ResidualBlock)[0m

[0;32m[VAIQ_NOTE]: =>Successfully convert 'ResidualBlock' to xmodel.(quantize_result/ResidualBlock_int.xmodel)[0m
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/quantizerimpl.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if inf.sum() > 0 or nan.sum() > 0:
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/fix_ops.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (isinstance(tensor, torch.Tensor) and
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/adaptive_avg_pool.py:41: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  input_size = [int(dim) for dim in input.shape[2:]]

[0;32m[VAIQ_NOTE]: ResidualBlock_int.pt is generated.(quantize_result/ResidualBlock_int.pt)[0m
Xmodel deployed
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input ResidualBlock::input_0
  warnings.warn(

[0;32m[VAIQ_NOTE]: ResidualBlock_int.onnx is generated.(quantize_result/ResidualBlock_int.onnx)[0m
-------- End of ResidualBlock-in_136x14x14-out_136-k_5-e_4-s_1-act_h_swish-use_se_True.pth test 
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: null
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B2304_0101000016010405
[UNILOG][INFO] Graph name: ResidualBlock, with op num: 57
[UNILOG][INFO] Begin to compile...
module_infer = 0, counter_m[module_infer] = 48, counter_p[module_infer] = 48
module_idx = 0, counter_m[module_idx] = 48, counter_p[module_idx] = 48
module_idx = 1, counter_m[module_idx] = 0, counter_p[module_idx] = 0
module_idx = 2, counter_m[module_idx] = 46, counter_p[module_idx] = 46
module_idx = 3, counter_m[module_idx] = 2, counter_p[module_idx] = 2
[0;33m[UNILOG][WARNING] subgraph_ResidualBlock__ResidualBlock_MBConvLayer_conv__Sequential_depth_conv__Conv2d_conv__ret_9 switch to no prefetch mode because of PM failure: .
[m[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/ResidualBlock-in_136x14x14-out_136-k_5-e_4-s_1-act_h_swish-use_se_True.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 057b3f03d7770449d0d3f7a69f0a3b96, and has been saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************



---------------------------Done Block 2-------------------------


-----------------------Strating Qt ResidualBlock-in_136 14 14-out_136-k_7-e_4-s_1-act_h_swish-use_se_True.pth -----------------------------

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_136x14x14-out_136-k_7-e_4-s_1-act_h_swish-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization calibration process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/17 [00:00<?, ?it/s]                                                  | 0/17 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–‰                                               | 1/17 [00:00<00:00, 5924.16it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                            | 2/17 [00:00<00:00, 2297.62it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 3/17 [00:00<00:00, 2408.21it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                      | 4/17 [00:00<00:00, 2635.03it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 5/17 [00:00<00:00, 2313.46it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 6/17 [00:00<00:00, 2494.88it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 7/17 [00:00<00:00, 2743.68it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1205, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 8/17 [00:00<00:00, 2824.92it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 9/17 [00:00<00:00, 2915.41it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 10/17 [00:00<00:00, 3001.94it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 | 11/17 [00:00<00:00, 3068.05it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 12/17 [00:00<00:00, 3040.27it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 13/17 [00:00<00:00, 2684.16it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 14/17 [00:00<00:00, 2553.39it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.27, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 15/17 [00:00<00:00, 2538.72it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16/17 [00:00<00:00, 2536.05it/s, OpInfo: name = return_0, type = Return]      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 2654.33it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 13
  0%|          | 0/13 [00:00<?, ?it/s]/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/torchquantizer.py:223: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  bnfp[1] = stats.mode(data)[0][0]
  8%|â–Š         | 1/13 [00:00<00:06,  1.85it/s] 15%|â–ˆâ–Œ        | 2/13 [00:01<00:05,  1.94it/s] 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:01<00:05,  1.96it/s] 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:02<00:04,  1.97it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:02<00:04,  1.97it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:03<00:03,  1.98it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:03<00:03,  1.99it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:04<00:02,  1.99it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:04<00:02,  1.97it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:05<00:01,  1.97it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:05<00:01,  1.99it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:06<00:00,  2.00it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:06<00:00,  2.06it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:06<00:00,  1.99it/s]

[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quantized/quant_info.json)[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0
-------- End of ResidualBlock-in_136x14x14-out_136-k_7-e_4-s_1-act_h_swish-use_se_True.pth test 
Compilation

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_136x14x14-out_136-k_7-e_4-s_1-act_h_swish-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization test process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/17 [00:00<?, ?it/s]                                                  | 0/17 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–‰                                               | 1/17 [00:00<00:00, 8924.05it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                            | 2/17 [00:00<00:00, 3151.24it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 3/17 [00:00<00:00, 3159.15it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                      | 4/17 [00:00<00:00, 3272.33it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 5/17 [00:00<00:00, 2838.98it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 6/17 [00:00<00:00, 2985.98it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 7/17 [00:00<00:00, 3189.24it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1205, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 8/17 [00:00<00:00, 3204.51it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 9/17 [00:00<00:00, 3248.04it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 10/17 [00:00<00:00, 3309.38it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 | 11/17 [00:00<00:00, 3353.00it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 12/17 [00:00<00:00, 3351.87it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 13/17 [00:00<00:00, 3250.43it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 14/17 [00:00<00:00, 3287.81it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.27, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 15/17 [00:00<00:00, 3336.76it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16/17 [00:00<00:00, 3295.63it/s, OpInfo: name = return_0, type = Return]      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 3433.81it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 1
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 28.78it/s]

[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0

[0;32m[VAIQ_NOTE]: =>Dumping 'ResidualBlock'' checking data...[0m

[0;32m[VAIQ_NOTE]: =>Finish dumping data.(quantize_result/deploy_check_data_int/ResidualBlock)[0m

[0;32m[VAIQ_NOTE]: =>Successfully convert 'ResidualBlock' to xmodel.(quantize_result/ResidualBlock_int.xmodel)[0m
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/quantizerimpl.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if inf.sum() > 0 or nan.sum() > 0:
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/fix_ops.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (isinstance(tensor, torch.Tensor) and
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/adaptive_avg_pool.py:41: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  input_size = [int(dim) for dim in input.shape[2:]]

[0;32m[VAIQ_NOTE]: ResidualBlock_int.pt is generated.(quantize_result/ResidualBlock_int.pt)[0m
Xmodel deployed
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input ResidualBlock::input_0
  warnings.warn(

[0;32m[VAIQ_NOTE]: ResidualBlock_int.onnx is generated.(quantize_result/ResidualBlock_int.onnx)[0m
-------- End of ResidualBlock-in_136x14x14-out_136-k_7-e_4-s_1-act_h_swish-use_se_True.pth test 
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: null
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B2304_0101000016010405
[UNILOG][INFO] Graph name: ResidualBlock, with op num: 57
[UNILOG][INFO] Begin to compile...
module_infer = 0, counter_m[module_infer] = 48, counter_p[module_infer] = 48
module_idx = 0, counter_m[module_idx] = 48, counter_p[module_idx] = 48
module_idx = 1, counter_m[module_idx] = 0, counter_p[module_idx] = 0
module_idx = 2, counter_m[module_idx] = 46, counter_p[module_idx] = 46
module_idx = 3, counter_m[module_idx] = 2, counter_p[module_idx] = 2
[0;33m[UNILOG][WARNING] subgraph_ResidualBlock__ResidualBlock_MBConvLayer_conv__Sequential_depth_conv__Conv2d_conv__ret_9 switch to no prefetch mode because of PM failure: .
[m[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/ResidualBlock-in_136x14x14-out_136-k_7-e_4-s_1-act_h_swish-use_se_True.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 0920b518e71aef89e2241c47e98d1233, and has been saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************



---------------------------Done Block 3-------------------------


-----------------------Strating Qt ResidualBlock-in_136 14 14-out_192-k_3-e_4-s_2-act_h_swish-use_se_True.pth -----------------------------

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_136x14x14-out_192-k_3-e_4-s_2-act_h_swish-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization calibration process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/16 [00:00<?, ?it/s]                                                  | 0/16 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–                                              | 1/16 [00:00<00:00, 6944.21it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 2/16 [00:00<00:00, 2340.57it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 3/16 [00:00<00:00, 2510.56it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 4/16 [00:00<00:00, 2728.45it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                  | 5/16 [00:00<00:00, 2152.25it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                               | 6/16 [00:00<00:00, 2326.51it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 7/16 [00:00<00:00, 2555.05it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1195, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 8/16 [00:00<00:00, 2661.36it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 9/16 [00:00<00:00, 2769.33it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 10/16 [00:00<00:00, 2869.27it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 11/16 [00:00<00:00, 2955.06it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 12/16 [00:00<00:00, 3031.84it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 13/16 [00:00<00:00, 2916.29it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 14/16 [00:00<00:00, 2986.03it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret, type = batch_norm]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 15/16 [00:00<00:00, 3061.24it/s, OpInfo: name = return_0, type = Return]                                                                        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 3197.33it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 13
  0%|          | 0/13 [00:00<?, ?it/s]/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/torchquantizer.py:223: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  bnfp[1] = stats.mode(data)[0][0]
  8%|â–Š         | 1/13 [00:00<00:04,  3.00it/s] 15%|â–ˆâ–Œ        | 2/13 [00:00<00:03,  3.23it/s] 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:00<00:02,  3.40it/s] 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:01<00:02,  3.50it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:01<00:02,  3.53it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:01<00:01,  3.57it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:02<00:01,  3.51it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:02<00:01,  3.50it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:02<00:01,  2.83it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:03<00:01,  2.93it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:03<00:00,  3.13it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:03<00:00,  3.27it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:03<00:00,  3.87it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:03<00:00,  3.40it/s]

[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quantized/quant_info.json)[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0
-------- End of ResidualBlock-in_136x14x14-out_192-k_3-e_4-s_2-act_h_swish-use_se_True.pth test 
Compilation

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_136x14x14-out_192-k_3-e_4-s_2-act_h_swish-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization test process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/16 [00:00<?, ?it/s]                                                  | 0/16 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–                                              | 1/16 [00:00<00:00, 9279.43it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 2/16 [00:00<00:00, 3157.17it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 3/16 [00:00<00:00, 3072.75it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 4/16 [00:00<00:00, 3226.39it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                  | 5/16 [00:00<00:00, 2805.55it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                               | 6/16 [00:00<00:00, 2952.35it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 7/16 [00:00<00:00, 3208.76it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1195, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 8/16 [00:00<00:00, 3245.74it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 9/16 [00:00<00:00, 3311.87it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 10/16 [00:00<00:00, 3376.78it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 11/16 [00:00<00:00, 3412.78it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 12/16 [00:00<00:00, 3410.00it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 13/16 [00:00<00:00, 3304.20it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 14/16 [00:00<00:00, 3347.03it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret, type = batch_norm]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 15/16 [00:00<00:00, 3400.24it/s, OpInfo: name = return_0, type = Return]                                                                        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 3549.98it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 1
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 66.02it/s]

[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0

[0;32m[VAIQ_NOTE]: =>Dumping 'ResidualBlock'' checking data...[0m

[0;32m[VAIQ_NOTE]: =>Finish dumping data.(quantize_result/deploy_check_data_int/ResidualBlock)[0m

[0;32m[VAIQ_NOTE]: =>Successfully convert 'ResidualBlock' to xmodel.(quantize_result/ResidualBlock_int.xmodel)[0m
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/quantizerimpl.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if inf.sum() > 0 or nan.sum() > 0:
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/fix_ops.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (isinstance(tensor, torch.Tensor) and
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/adaptive_avg_pool.py:41: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  input_size = [int(dim) for dim in input.shape[2:]]

[0;32m[VAIQ_NOTE]: ResidualBlock_int.pt is generated.(quantize_result/ResidualBlock_int.pt)[0m
Xmodel deployed
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input ResidualBlock::input_0
  warnings.warn(

[0;32m[VAIQ_NOTE]: ResidualBlock_int.onnx is generated.(quantize_result/ResidualBlock_int.onnx)[0m
-------- End of ResidualBlock-in_136x14x14-out_192-k_3-e_4-s_2-act_h_swish-use_se_True.pth test 
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: null
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B2304_0101000016010405
[UNILOG][INFO] Graph name: ResidualBlock, with op num: 55
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/ResidualBlock-in_136x14x14-out_192-k_3-e_4-s_2-act_h_swish-use_se_True.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 05b5b4a8e094adb8a3a63cb67052c199, and has been saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************



---------------------------Done Block 4-------------------------


-----------------------Strating Qt ResidualBlock-in_136 14 14-out_192-k_5-e_4-s_2-act_h_swish-use_se_True.pth -----------------------------

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_136x14x14-out_192-k_5-e_4-s_2-act_h_swish-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization calibration process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/16 [00:00<?, ?it/s]                                                  | 0/16 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–                                              | 1/16 [00:00<00:00, 10034.22it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 2/16 [00:00<00:00, 3313.04it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 3/16 [00:00<00:00, 3238.84it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 4/16 [00:00<00:00, 3323.54it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                  | 5/16 [00:00<00:00, 2895.02it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                               | 6/16 [00:00<00:00, 3032.03it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 7/16 [00:00<00:00, 3280.46it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1195, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 8/16 [00:00<00:00, 3310.42it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 9/16 [00:00<00:00, 3365.31it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 10/16 [00:00<00:00, 3429.80it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 11/16 [00:00<00:00, 3468.97it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 12/16 [00:00<00:00, 3505.48it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 13/16 [00:00<00:00, 3383.76it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 14/16 [00:00<00:00, 3418.54it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret, type = batch_norm]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 15/16 [00:00<00:00, 3478.63it/s, OpInfo: name = return_0, type = Return]                                                                        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 3634.38it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 13
  0%|          | 0/13 [00:00<?, ?it/s]/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/torchquantizer.py:223: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  bnfp[1] = stats.mode(data)[0][0]
  8%|â–Š         | 1/13 [00:00<00:04,  2.84it/s] 15%|â–ˆâ–Œ        | 2/13 [00:00<00:03,  3.00it/s] 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:00<00:03,  3.04it/s] 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:01<00:02,  3.03it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:01<00:02,  3.03it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:01<00:02,  3.12it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:02<00:01,  3.20it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:02<00:01,  3.37it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:02<00:01,  3.38it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:03<00:00,  3.41it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:03<00:00,  3.36it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:03<00:00,  3.45it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:03<00:00,  4.12it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:03<00:00,  3.41it/s]

[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quantized/quant_info.json)[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0
-------- End of ResidualBlock-in_136x14x14-out_192-k_5-e_4-s_2-act_h_swish-use_se_True.pth test 
Compilation

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_136x14x14-out_192-k_5-e_4-s_2-act_h_swish-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization test process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/16 [00:00<?, ?it/s]                                                  | 0/16 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–                                              | 1/16 [00:00<00:00, 9446.63it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 2/16 [00:00<00:00, 3404.47it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 3/16 [00:00<00:00, 3291.37it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 4/16 [00:00<00:00, 3386.60it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                  | 5/16 [00:00<00:00, 2931.85it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                               | 6/16 [00:00<00:00, 3059.30it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 7/16 [00:00<00:00, 3295.93it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1195, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 8/16 [00:00<00:00, 3323.21it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 9/16 [00:00<00:00, 3344.44it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 10/16 [00:00<00:00, 3420.85it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 11/16 [00:00<00:00, 3468.45it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 12/16 [00:00<00:00, 3467.80it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 13/16 [00:00<00:00, 3348.64it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 14/16 [00:00<00:00, 3368.92it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret, type = batch_norm]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 15/16 [00:00<00:00, 3419.64it/s, OpInfo: name = return_0, type = Return]                                                                        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 3573.61it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 1
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 64.29it/s]

[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0

[0;32m[VAIQ_NOTE]: =>Dumping 'ResidualBlock'' checking data...[0m

[0;32m[VAIQ_NOTE]: =>Finish dumping data.(quantize_result/deploy_check_data_int/ResidualBlock)[0m

[0;32m[VAIQ_NOTE]: =>Successfully convert 'ResidualBlock' to xmodel.(quantize_result/ResidualBlock_int.xmodel)[0m
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/quantizerimpl.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if inf.sum() > 0 or nan.sum() > 0:
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/fix_ops.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (isinstance(tensor, torch.Tensor) and
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/adaptive_avg_pool.py:41: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  input_size = [int(dim) for dim in input.shape[2:]]

[0;32m[VAIQ_NOTE]: ResidualBlock_int.pt is generated.(quantize_result/ResidualBlock_int.pt)[0m
Xmodel deployed
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input ResidualBlock::input_0
  warnings.warn(

[0;32m[VAIQ_NOTE]: ResidualBlock_int.onnx is generated.(quantize_result/ResidualBlock_int.onnx)[0m
-------- End of ResidualBlock-in_136x14x14-out_192-k_5-e_4-s_2-act_h_swish-use_se_True.pth test 
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: null
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B2304_0101000016010405
[UNILOG][INFO] Graph name: ResidualBlock, with op num: 55
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/ResidualBlock-in_136x14x14-out_192-k_5-e_4-s_2-act_h_swish-use_se_True.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is b3df49ccd4e98c8e44945749c0963cf0, and has been saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************



---------------------------Done Block 5-------------------------


-----------------------Strating Qt ResidualBlock-in_136 14 14-out_192-k_7-e_4-s_2-act_h_swish-use_se_True.pth -----------------------------

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_136x14x14-out_192-k_7-e_4-s_2-act_h_swish-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization calibration process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/16 [00:00<?, ?it/s]                                                  | 0/16 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–                                              | 1/16 [00:00<00:00, 9709.04it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 2/16 [00:00<00:00, 3185.95it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 3/16 [00:00<00:00, 3173.50it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 4/16 [00:00<00:00, 3322.88it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                  | 5/16 [00:00<00:00, 2887.05it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                               | 6/16 [00:00<00:00, 3046.71it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 7/16 [00:00<00:00, 3287.07it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1195, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 8/16 [00:00<00:00, 3297.41it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 9/16 [00:00<00:00, 3364.41it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 10/16 [00:00<00:00, 3426.44it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 11/16 [00:00<00:00, 3446.69it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 12/16 [00:00<00:00, 3484.36it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 13/16 [00:00<00:00, 3377.47it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 14/16 [00:00<00:00, 3412.38it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret, type = batch_norm]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 15/16 [00:00<00:00, 3463.31it/s, OpInfo: name = return_0, type = Return]                                                                        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 3614.03it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 13
  0%|          | 0/13 [00:00<?, ?it/s]/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/torchquantizer.py:223: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  bnfp[1] = stats.mode(data)[0][0]
  8%|â–Š         | 1/13 [00:00<00:03,  3.29it/s] 15%|â–ˆâ–Œ        | 2/13 [00:00<00:03,  3.53it/s] 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:00<00:02,  3.62it/s] 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:01<00:02,  3.66it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:01<00:02,  3.67it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:01<00:01,  3.69it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:01<00:01,  3.72it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:02<00:01,  3.72it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:02<00:01,  3.74it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:02<00:00,  3.72it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:02<00:00,  3.74it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:03<00:00,  3.76it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:03<00:00,  4.41it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:03<00:00,  3.84it/s]

[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quantized/quant_info.json)[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0
-------- End of ResidualBlock-in_136x14x14-out_192-k_7-e_4-s_2-act_h_swish-use_se_True.pth test 
Compilation

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_136x14x14-out_192-k_7-e_4-s_2-act_h_swish-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization test process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/16 [00:00<?, ?it/s]                                                  | 0/16 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–                                              | 1/16 [00:00<00:00, 9425.40it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 2/16 [00:00<00:00, 3200.54it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 3/16 [00:00<00:00, 3088.59it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 4/16 [00:00<00:00, 3221.43it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                  | 5/16 [00:00<00:00, 2809.31it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                               | 6/16 [00:00<00:00, 2957.21it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 7/16 [00:00<00:00, 3212.27it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1195, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 8/16 [00:00<00:00, 3230.11it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 9/16 [00:00<00:00, 3272.26it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 10/16 [00:00<00:00, 3333.57it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 11/16 [00:00<00:00, 3368.43it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 12/16 [00:00<00:00, 3380.46it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 13/16 [00:00<00:00, 3278.77it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 14/16 [00:00<00:00, 3319.97it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret, type = batch_norm]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 15/16 [00:00<00:00, 3381.23it/s, OpInfo: name = return_0, type = Return]                                                                        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 3533.91it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 1
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 47.83it/s]

[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0

[0;32m[VAIQ_NOTE]: =>Dumping 'ResidualBlock'' checking data...[0m

[0;32m[VAIQ_NOTE]: =>Finish dumping data.(quantize_result/deploy_check_data_int/ResidualBlock)[0m

[0;32m[VAIQ_NOTE]: =>Successfully convert 'ResidualBlock' to xmodel.(quantize_result/ResidualBlock_int.xmodel)[0m
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/quantizerimpl.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if inf.sum() > 0 or nan.sum() > 0:
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/fix_ops.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (isinstance(tensor, torch.Tensor) and
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/adaptive_avg_pool.py:41: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  input_size = [int(dim) for dim in input.shape[2:]]

[0;32m[VAIQ_NOTE]: ResidualBlock_int.pt is generated.(quantize_result/ResidualBlock_int.pt)[0m
Xmodel deployed
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input ResidualBlock::input_0
  warnings.warn(

[0;32m[VAIQ_NOTE]: ResidualBlock_int.onnx is generated.(quantize_result/ResidualBlock_int.onnx)[0m
-------- End of ResidualBlock-in_136x14x14-out_192-k_7-e_4-s_2-act_h_swish-use_se_True.pth test 
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: null
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B2304_0101000016010405
[UNILOG][INFO] Graph name: ResidualBlock, with op num: 55
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/ResidualBlock-in_136x14x14-out_192-k_7-e_4-s_2-act_h_swish-use_se_True.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is d676060448bb63b5614605a935c2189d, and has been saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************



---------------------------Done Block 6-------------------------


-----------------------Strating Qt ResidualBlock-in_192 7 7-out_192-k_3-e_4-s_1-act_h_swish-use_se_True.pth -----------------------------

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_192x7x7-out_192-k_3-e_4-s_1-act_h_swish-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization calibration process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/17 [00:00<?, ?it/s]                                                  | 0/17 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–‰                                               | 1/17 [00:00<00:00, 9320.68it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                            | 2/17 [00:00<00:00, 3160.74it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 3/17 [00:00<00:00, 3121.54it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                      | 4/17 [00:00<00:00, 3226.39it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 5/17 [00:00<00:00, 2813.08it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 6/17 [00:00<00:00, 2973.28it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 7/17 [00:00<00:00, 3235.28it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1205, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 8/17 [00:00<00:00, 3282.25it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 9/17 [00:00<00:00, 3350.38it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 10/17 [00:00<00:00, 3421.69it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 | 11/17 [00:00<00:00, 3455.72it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 12/17 [00:00<00:00, 3481.95it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 13/17 [00:00<00:00, 3357.51it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 14/17 [00:00<00:00, 3388.16it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.27, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 15/17 [00:00<00:00, 3439.65it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16/17 [00:00<00:00, 3371.46it/s, OpInfo: name = return_0, type = Return]      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 3503.50it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 13
  0%|          | 0/13 [00:00<?, ?it/s]/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/torchquantizer.py:223: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  bnfp[1] = stats.mode(data)[0][0]
  8%|â–Š         | 1/13 [00:00<00:02,  4.25it/s] 15%|â–ˆâ–Œ        | 2/13 [00:00<00:02,  4.93it/s] 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:00<00:01,  5.24it/s] 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:00<00:01,  5.37it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:00<00:01,  5.43it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:01<00:01,  5.46it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:01<00:01,  5.50it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:01<00:00,  5.52it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:01<00:00,  5.55it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:01<00:00,  5.54it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:02<00:00,  5.56it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:02<00:00,  5.53it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:02<00:00,  5.63it/s]

[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quantized/quant_info.json)[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0
-------- End of ResidualBlock-in_192x7x7-out_192-k_3-e_4-s_1-act_h_swish-use_se_True.pth test 
Compilation

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_192x7x7-out_192-k_3-e_4-s_1-act_h_swish-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization test process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/17 [00:00<?, ?it/s]                                                  | 0/17 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–‰                                               | 1/17 [00:00<00:00, 9404.27it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                            | 2/17 [00:00<00:00, 3264.05it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 3/17 [00:00<00:00, 3292.23it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                      | 4/17 [00:00<00:00, 3435.84it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 5/17 [00:00<00:00, 2903.03it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 6/17 [00:00<00:00, 3035.68it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 7/17 [00:00<00:00, 3278.26it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1205, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 8/17 [00:00<00:00, 3259.93it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 9/17 [00:00<00:00, 3165.25it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 10/17 [00:00<00:00, 3225.40it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 | 11/17 [00:00<00:00, 3263.13it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 12/17 [00:00<00:00, 3301.52it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 13/17 [00:00<00:00, 3204.96it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 14/17 [00:00<00:00, 3245.65it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.27, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 15/17 [00:00<00:00, 3295.51it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16/17 [00:00<00:00, 3241.82it/s, OpInfo: name = return_0, type = Return]      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 3377.38it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 1
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 67.64it/s]

[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0

[0;32m[VAIQ_NOTE]: =>Dumping 'ResidualBlock'' checking data...[0m

[0;32m[VAIQ_NOTE]: =>Finish dumping data.(quantize_result/deploy_check_data_int/ResidualBlock)[0m

[0;32m[VAIQ_NOTE]: =>Successfully convert 'ResidualBlock' to xmodel.(quantize_result/ResidualBlock_int.xmodel)[0m
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/quantizerimpl.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if inf.sum() > 0 or nan.sum() > 0:
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/fix_ops.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (isinstance(tensor, torch.Tensor) and
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/adaptive_avg_pool.py:41: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  input_size = [int(dim) for dim in input.shape[2:]]

[0;32m[VAIQ_NOTE]: ResidualBlock_int.pt is generated.(quantize_result/ResidualBlock_int.pt)[0m
Xmodel deployed
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input ResidualBlock::input_0
  warnings.warn(

[0;32m[VAIQ_NOTE]: ResidualBlock_int.onnx is generated.(quantize_result/ResidualBlock_int.onnx)[0m
-------- End of ResidualBlock-in_192x7x7-out_192-k_3-e_4-s_1-act_h_swish-use_se_True.pth test 
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: null
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B2304_0101000016010405
[UNILOG][INFO] Graph name: ResidualBlock, with op num: 57
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/ResidualBlock-in_192x7x7-out_192-k_3-e_4-s_1-act_h_swish-use_se_True.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 62506014444a64e3d1b7ab4e25c242c8, and has been saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************



---------------------------Done Block 7-------------------------


-----------------------Strating Qt ResidualBlock-in_192 7 7-out_192-k_5-e_4-s_1-act_h_swish-use_se_True.pth -----------------------------

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_192x7x7-out_192-k_5-e_4-s_1-act_h_swish-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization calibration process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/17 [00:00<?, ?it/s]                                                  | 0/17 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–‰                                               | 1/17 [00:00<00:00, 8738.13it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                            | 2/17 [00:00<00:00, 3238.84it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 3/17 [00:00<00:00, 3163.92it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                      | 4/17 [00:00<00:00, 3241.97it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 5/17 [00:00<00:00, 2810.44it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 6/17 [00:00<00:00, 2936.16it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 7/17 [00:00<00:00, 3179.57it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1205, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 8/17 [00:00<00:00, 3204.82it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 9/17 [00:00<00:00, 3260.67it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 10/17 [00:00<00:00, 3322.75it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 | 11/17 [00:00<00:00, 3357.64it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 12/17 [00:00<00:00, 3375.47it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 13/17 [00:00<00:00, 3267.77it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 14/17 [00:00<00:00, 3303.72it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.27, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 15/17 [00:00<00:00, 3348.83it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16/17 [00:00<00:00, 3300.65it/s, OpInfo: name = return_0, type = Return]      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 3435.30it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 13
  0%|          | 0/13 [00:00<?, ?it/s]/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/torchquantizer.py:223: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  bnfp[1] = stats.mode(data)[0][0]
  8%|â–Š         | 1/13 [00:00<00:02,  4.25it/s] 15%|â–ˆâ–Œ        | 2/13 [00:00<00:02,  4.89it/s] 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:00<00:01,  5.19it/s] 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:00<00:01,  5.32it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:00<00:01,  5.42it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:01<00:01,  5.45it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:01<00:01,  5.46it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:01<00:00,  5.49it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:01<00:00,  5.54it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:01<00:00,  5.53it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:02<00:00,  5.55it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:02<00:00,  5.56it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:02<00:00,  6.34it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:02<00:00,  5.60it/s]

[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quantized/quant_info.json)[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0
-------- End of ResidualBlock-in_192x7x7-out_192-k_5-e_4-s_1-act_h_swish-use_se_True.pth test 
Compilation

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_192x7x7-out_192-k_5-e_4-s_1-act_h_swish-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization test process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/17 [00:00<?, ?it/s]                                                  | 0/17 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–‰                                               | 1/17 [00:00<00:00, 8756.38it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                            | 2/17 [00:00<00:00, 3211.57it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 3/17 [00:00<00:00, 3213.21it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                      | 4/17 [00:00<00:00, 3315.00it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 5/17 [00:00<00:00, 2864.96it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 6/17 [00:00<00:00, 2998.79it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 7/17 [00:00<00:00, 3255.00it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1205, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 8/17 [00:00<00:00, 3247.94it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 9/17 [00:00<00:00, 3271.12it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 10/17 [00:00<00:00, 3342.07it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 | 11/17 [00:00<00:00, 3384.24it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 12/17 [00:00<00:00, 3432.80it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 13/17 [00:00<00:00, 3314.45it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 14/17 [00:00<00:00, 3346.26it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.27, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 15/17 [00:00<00:00, 3402.99it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16/17 [00:00<00:00, 3355.61it/s, OpInfo: name = return_0, type = Return]      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 3495.77it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 1
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 65.47it/s]

[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0

[0;32m[VAIQ_NOTE]: =>Dumping 'ResidualBlock'' checking data...[0m

[0;32m[VAIQ_NOTE]: =>Finish dumping data.(quantize_result/deploy_check_data_int/ResidualBlock)[0m

[0;32m[VAIQ_NOTE]: =>Successfully convert 'ResidualBlock' to xmodel.(quantize_result/ResidualBlock_int.xmodel)[0m
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/quantizerimpl.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if inf.sum() > 0 or nan.sum() > 0:
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/fix_ops.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (isinstance(tensor, torch.Tensor) and
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/adaptive_avg_pool.py:41: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  input_size = [int(dim) for dim in input.shape[2:]]

[0;32m[VAIQ_NOTE]: ResidualBlock_int.pt is generated.(quantize_result/ResidualBlock_int.pt)[0m
Xmodel deployed
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input ResidualBlock::input_0
  warnings.warn(

[0;32m[VAIQ_NOTE]: ResidualBlock_int.onnx is generated.(quantize_result/ResidualBlock_int.onnx)[0m
-------- End of ResidualBlock-in_192x7x7-out_192-k_5-e_4-s_1-act_h_swish-use_se_True.pth test 
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: null
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B2304_0101000016010405
[UNILOG][INFO] Graph name: ResidualBlock, with op num: 57
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/ResidualBlock-in_192x7x7-out_192-k_5-e_4-s_1-act_h_swish-use_se_True.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 6b325bc4a53cc09f5ce639861641700c, and has been saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************



---------------------------Done Block 8-------------------------


-----------------------Strating Qt ResidualBlock-in_192 7 7-out_192-k_7-e_4-s_1-act_h_swish-use_se_True.pth -----------------------------

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_192x7x7-out_192-k_7-e_4-s_1-act_h_swish-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization calibration process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/17 [00:00<?, ?it/s]                                                  | 0/17 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–‰                                               | 1/17 [00:00<00:00, 5785.25it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                            | 2/17 [00:00<00:00, 2343.84it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 3/17 [00:00<00:00, 2366.10it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                      | 4/17 [00:00<00:00, 2484.78it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 5/17 [00:00<00:00, 2208.92it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 6/17 [00:00<00:00, 2310.91it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 7/17 [00:00<00:00, 2507.91it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1205, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 8/17 [00:00<00:00, 2532.03it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 9/17 [00:00<00:00, 2574.60it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 10/17 [00:00<00:00, 2623.41it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 | 11/17 [00:00<00:00, 2656.61it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 12/17 [00:00<00:00, 2709.35it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 13/17 [00:00<00:00, 2650.62it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 14/17 [00:00<00:00, 2695.82it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.27, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 15/17 [00:00<00:00, 2753.61it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16/17 [00:00<00:00, 2741.15it/s, OpInfo: name = return_0, type = Return]      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 2857.96it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 13
  0%|          | 0/13 [00:00<?, ?it/s]/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/torchquantizer.py:223: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  bnfp[1] = stats.mode(data)[0][0]
  8%|â–Š         | 1/13 [00:00<00:08,  1.42it/s] 15%|â–ˆâ–Œ        | 2/13 [00:00<00:04,  2.51it/s] 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:01<00:03,  3.33it/s] 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:01<00:02,  3.92it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:01<00:01,  4.35it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:01<00:01,  4.65it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:01<00:01,  4.87it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:01<00:00,  5.02it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:02<00:00,  5.15it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:02<00:00,  5.27it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:02<00:00,  5.31it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:02<00:00,  5.35it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:02<00:00,  4.59it/s]

[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quantized/quant_info.json)[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0
-------- End of ResidualBlock-in_192x7x7-out_192-k_7-e_4-s_1-act_h_swish-use_se_True.pth test 
Compilation

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_192x7x7-out_192-k_7-e_4-s_1-act_h_swish-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization test process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/17 [00:00<?, ?it/s]                                                  | 0/17 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–‰                                               | 1/17 [00:00<00:00, 9845.78it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                            | 2/17 [00:00<00:00, 3228.87it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 3/17 [00:00<00:00, 3120.76it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                      | 4/17 [00:00<00:00, 3206.65it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 5/17 [00:00<00:00, 2811.95it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 6/17 [00:00<00:00, 2957.90it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 7/17 [00:00<00:00, 3203.86it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1205, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 8/17 [00:00<00:00, 3161.04it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 9/17 [00:00<00:00, 3076.26it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 10/17 [00:00<00:00, 3155.27it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 | 11/17 [00:00<00:00, 3215.37it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 12/17 [00:00<00:00, 3253.50it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 13/17 [00:00<00:00, 3168.09it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 14/17 [00:00<00:00, 3210.69it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.27, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 15/17 [00:00<00:00, 3275.61it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16/17 [00:00<00:00, 3220.20it/s, OpInfo: name = return_0, type = Return]      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 3354.65it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 1
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 66.21it/s]

[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0

[0;32m[VAIQ_NOTE]: =>Dumping 'ResidualBlock'' checking data...[0m

[0;32m[VAIQ_NOTE]: =>Finish dumping data.(quantize_result/deploy_check_data_int/ResidualBlock)[0m

[0;32m[VAIQ_NOTE]: =>Successfully convert 'ResidualBlock' to xmodel.(quantize_result/ResidualBlock_int.xmodel)[0m
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/quantizerimpl.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if inf.sum() > 0 or nan.sum() > 0:
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/fix_ops.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (isinstance(tensor, torch.Tensor) and
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/adaptive_avg_pool.py:41: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  input_size = [int(dim) for dim in input.shape[2:]]

[0;32m[VAIQ_NOTE]: ResidualBlock_int.pt is generated.(quantize_result/ResidualBlock_int.pt)[0m
Xmodel deployed
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input ResidualBlock::input_0
  warnings.warn(

[0;32m[VAIQ_NOTE]: ResidualBlock_int.onnx is generated.(quantize_result/ResidualBlock_int.onnx)[0m
-------- End of ResidualBlock-in_192x7x7-out_192-k_7-e_4-s_1-act_h_swish-use_se_True.pth test 
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: null
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B2304_0101000016010405
[UNILOG][INFO] Graph name: ResidualBlock, with op num: 57
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/ResidualBlock-in_192x7x7-out_192-k_7-e_4-s_1-act_h_swish-use_se_True.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 94d19246fffed4aaa64d27d0d327602e, and has been saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************



---------------------------Done Block 9-------------------------


-----------------------Strating Qt ResidualBlock-in_32 56 56-out_32-k_3-e_6-s_1-act_relu-use_se_False.pth -----------------------------

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_32x56x56-out_32-k_3-e_6-s_1-act_relu-use_se_False.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization calibration process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/11 [00:00<?, ?it/s]                                                  | 0/11 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                             | 1/11 [00:00<00:00, 9799.78it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                         | 2/11 [00:00<00:00, 3275.52it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                    | 3/11 [00:00<00:00, 3204.20it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/ReLU[act]/ret.7, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 4/11 [00:00<00:00, 3416.25it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                           | 5/11 [00:00<00:00, 3548.48it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                      | 6/11 [00:00<00:00, 3653.04it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/ReLU[act]/ret.13, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 7/11 [00:00<00:00, 3937.26it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.15, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž             | 8/11 [00:00<00:00, 4006.50it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.17, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 9/11 [00:00<00:00, 4079.18it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 10/11 [00:00<00:00, 3836.72it/s, OpInfo: name = return_0, type = Return]     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 3987.33it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 13
  0%|          | 0/13 [00:00<?, ?it/s]/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/torchquantizer.py:223: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  bnfp[1] = stats.mode(data)[0][0]
  8%|â–Š         | 1/13 [00:01<00:11,  1.00it/s] 15%|â–ˆâ–Œ        | 2/13 [00:01<00:10,  1.03it/s] 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:02<00:09,  1.02it/s] 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:03<00:08,  1.02it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:04<00:07,  1.02it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:05<00:06,  1.02it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:06<00:05,  1.02it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:07<00:04,  1.03it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:08<00:03,  1.04it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:09<00:02,  1.05it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:10<00:01,  1.05it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:11<00:00,  1.06it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:12<00:00,  1.26it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:12<00:00,  1.08it/s]

[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quantized/quant_info.json)[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0
-------- End of ResidualBlock-in_32x56x56-out_32-k_3-e_6-s_1-act_relu-use_se_False.pth test 
Compilation

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_32x56x56-out_32-k_3-e_6-s_1-act_relu-use_se_False.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization test process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/11 [00:00<?, ?it/s]                                                  | 0/11 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                             | 1/11 [00:00<00:00, 10305.42it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                         | 2/11 [00:00<00:00, 3211.57it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                    | 3/11 [00:00<00:00, 3130.86it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/ReLU[act]/ret.7, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 4/11 [00:00<00:00, 3341.41it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                           | 5/11 [00:00<00:00, 3452.67it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                      | 6/11 [00:00<00:00, 3606.97it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/ReLU[act]/ret.13, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 7/11 [00:00<00:00, 3917.82it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.15, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž             | 8/11 [00:00<00:00, 3970.94it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.17, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 9/11 [00:00<00:00, 4041.62it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 10/11 [00:00<00:00, 3782.06it/s, OpInfo: name = return_0, type = Return]     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 3935.96it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 1
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 36.33it/s]

[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0

[0;32m[VAIQ_NOTE]: =>Dumping 'ResidualBlock'' checking data...[0m

[0;32m[VAIQ_NOTE]: =>Finish dumping data.(quantize_result/deploy_check_data_int/ResidualBlock)[0m

[0;32m[VAIQ_NOTE]: =>Successfully convert 'ResidualBlock' to xmodel.(quantize_result/ResidualBlock_int.xmodel)[0m
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/quantizerimpl.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if inf.sum() > 0 or nan.sum() > 0:
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/fix_ops.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (isinstance(tensor, torch.Tensor) and

[0;32m[VAIQ_NOTE]: ResidualBlock_int.pt is generated.(quantize_result/ResidualBlock_int.pt)[0m
Xmodel deployed
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input ResidualBlock::input_0
  warnings.warn(

[0;32m[VAIQ_NOTE]: ResidualBlock_int.onnx is generated.(quantize_result/ResidualBlock_int.onnx)[0m
-------- End of ResidualBlock-in_32x56x56-out_32-k_3-e_6-s_1-act_relu-use_se_False.pth test 
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: null
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B2304_0101000016010405
[UNILOG][INFO] Graph name: ResidualBlock, with op num: 24
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/ResidualBlock-in_32x56x56-out_32-k_3-e_6-s_1-act_relu-use_se_False.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 4695d7e99f41909f2e8ab6f35e8c4f54, and has been saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************



---------------------------Done Block 10-------------------------


-----------------------Strating Qt ResidualBlock-in_32 56 56-out_32-k_5-e_6-s_1-act_relu-use_se_False.pth -----------------------------

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_32x56x56-out_32-k_5-e_6-s_1-act_relu-use_se_False.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization calibration process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/11 [00:00<?, ?it/s]                                                  | 0/11 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                             | 1/11 [00:00<00:00, 9320.68it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                         | 2/11 [00:00<00:00, 3256.45it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                    | 3/11 [00:00<00:00, 3238.01it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/ReLU[act]/ret.7, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 4/11 [00:00<00:00, 3472.82it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                           | 5/11 [00:00<00:00, 3569.62it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                      | 6/11 [00:00<00:00, 3693.79it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/ReLU[act]/ret.13, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 7/11 [00:00<00:00, 3975.64it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.15, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž             | 8/11 [00:00<00:00, 4035.41it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.17, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 9/11 [00:00<00:00, 4082.71it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 10/11 [00:00<00:00, 3824.83it/s, OpInfo: name = return_0, type = Return]     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 3976.67it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 13
  0%|          | 0/13 [00:00<?, ?it/s]/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/torchquantizer.py:223: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  bnfp[1] = stats.mode(data)[0][0]
  8%|â–Š         | 1/13 [00:01<00:12,  1.03s/it] 15%|â–ˆâ–Œ        | 2/13 [00:02<00:11,  1.00s/it] 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:02<00:09,  1.01it/s] 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:03<00:08,  1.01it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:04<00:07,  1.01it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:05<00:06,  1.00it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:06<00:05,  1.01it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:07<00:04,  1.01it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:08<00:03,  1.02it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:09<00:02,  1.02it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:10<00:01,  1.02it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:11<00:00,  1.03it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:12<00:00,  1.22it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:12<00:00,  1.06it/s]

[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quantized/quant_info.json)[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0
-------- End of ResidualBlock-in_32x56x56-out_32-k_5-e_6-s_1-act_relu-use_se_False.pth test 
Compilation

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_32x56x56-out_32-k_5-e_6-s_1-act_relu-use_se_False.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization test process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/11 [00:00<?, ?it/s]                                                  | 0/11 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                             | 1/11 [00:00<00:00, 9868.95it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                         | 2/11 [00:00<00:00, 3202.98it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                    | 3/11 [00:00<00:00, 3216.49it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/ReLU[act]/ret.7, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 4/11 [00:00<00:00, 3439.36it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                           | 5/11 [00:00<00:00, 3543.08it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                      | 6/11 [00:00<00:00, 3650.40it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/ReLU[act]/ret.13, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 7/11 [00:00<00:00, 3955.29it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.15, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž             | 8/11 [00:00<00:00, 4019.94it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.17, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 9/11 [00:00<00:00, 4085.36it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 10/11 [00:00<00:00, 3870.00it/s, OpInfo: name = return_0, type = Return]     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 4029.11it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 1
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 37.22it/s]

[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0

[0;32m[VAIQ_NOTE]: =>Dumping 'ResidualBlock'' checking data...[0m

[0;32m[VAIQ_NOTE]: =>Finish dumping data.(quantize_result/deploy_check_data_int/ResidualBlock)[0m

[0;32m[VAIQ_NOTE]: =>Successfully convert 'ResidualBlock' to xmodel.(quantize_result/ResidualBlock_int.xmodel)[0m
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/quantizerimpl.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if inf.sum() > 0 or nan.sum() > 0:
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/fix_ops.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (isinstance(tensor, torch.Tensor) and

[0;32m[VAIQ_NOTE]: ResidualBlock_int.pt is generated.(quantize_result/ResidualBlock_int.pt)[0m
Xmodel deployed
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input ResidualBlock::input_0
  warnings.warn(

[0;32m[VAIQ_NOTE]: ResidualBlock_int.onnx is generated.(quantize_result/ResidualBlock_int.onnx)[0m
-------- End of ResidualBlock-in_32x56x56-out_32-k_5-e_6-s_1-act_relu-use_se_False.pth test 
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: null
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B2304_0101000016010405
[UNILOG][INFO] Graph name: ResidualBlock, with op num: 24
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/ResidualBlock-in_32x56x56-out_32-k_5-e_6-s_1-act_relu-use_se_False.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 4a69817edabdc4d23ad1d3b7a9d6a60a, and has been saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************



---------------------------Done Block 11-------------------------


-----------------------Strating Qt ResidualBlock-in_32 56 56-out_32-k_7-e_6-s_1-act_relu-use_se_False.pth -----------------------------

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_32x56x56-out_32-k_7-e_6-s_1-act_relu-use_se_False.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization calibration process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/11 [00:00<?, ?it/s]                                                  | 0/11 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                             | 1/11 [00:00<00:00, 9776.93it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                         | 2/11 [00:00<00:00, 3220.20it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                    | 3/11 [00:00<00:00, 3148.88it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/ReLU[act]/ret.7, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 4/11 [00:00<00:00, 3324.20it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                           | 5/11 [00:00<00:00, 3428.40it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                      | 6/11 [00:00<00:00, 3548.48it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/ReLU[act]/ret.13, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 7/11 [00:00<00:00, 3845.47it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.15, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž             | 8/11 [00:00<00:00, 3886.31it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.17, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 9/11 [00:00<00:00, 3942.02it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 10/11 [00:00<00:00, 3726.61it/s, OpInfo: name = return_0, type = Return]     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 3868.31it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 13
  0%|          | 0/13 [00:00<?, ?it/s]/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/torchquantizer.py:223: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  bnfp[1] = stats.mode(data)[0][0]
  8%|â–Š         | 1/13 [00:01<00:12,  1.00s/it] 15%|â–ˆâ–Œ        | 2/13 [00:01<00:10,  1.00it/s] 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:03<00:12,  1.28s/it] 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:04<00:10,  1.17s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:05<00:08,  1.11s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:06<00:07,  1.09s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:07<00:06,  1.07s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:08<00:05,  1.04s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:09<00:04,  1.02s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:10<00:03,  1.02s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:11<00:02,  1.01s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:12<00:01,  1.00s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:13<00:00,  1.19it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:13<00:00,  1.01s/it]

[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quantized/quant_info.json)[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0
-------- End of ResidualBlock-in_32x56x56-out_32-k_7-e_6-s_1-act_relu-use_se_False.pth test 
Compilation

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_32x56x56-out_32-k_7-e_6-s_1-act_relu-use_se_False.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization test process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/11 [00:00<?, ?it/s]                                                  | 0/11 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                             | 1/11 [00:00<00:00, 10034.22it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                         | 2/11 [00:00<00:00, 3253.92it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                    | 3/11 [00:00<00:00, 3200.13it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/ReLU[act]/ret.7, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 4/11 [00:00<00:00, 3381.82it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                           | 5/11 [00:00<00:00, 3500.50it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                      | 6/11 [00:00<00:00, 3625.15it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/ReLU[act]/ret.13, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 7/11 [00:00<00:00, 3944.13it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.15, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž             | 8/11 [00:00<00:00, 3977.53it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.17, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 9/11 [00:00<00:00, 4024.39it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 10/11 [00:00<00:00, 3780.01it/s, OpInfo: name = return_0, type = Return]     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 3934.62it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 1
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.97it/s]

[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0

[0;32m[VAIQ_NOTE]: =>Dumping 'ResidualBlock'' checking data...[0m

[0;32m[VAIQ_NOTE]: =>Finish dumping data.(quantize_result/deploy_check_data_int/ResidualBlock)[0m

[0;32m[VAIQ_NOTE]: =>Successfully convert 'ResidualBlock' to xmodel.(quantize_result/ResidualBlock_int.xmodel)[0m
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/quantizerimpl.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if inf.sum() > 0 or nan.sum() > 0:
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/fix_ops.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (isinstance(tensor, torch.Tensor) and

[0;32m[VAIQ_NOTE]: ResidualBlock_int.pt is generated.(quantize_result/ResidualBlock_int.pt)[0m
Xmodel deployed
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input ResidualBlock::input_0
  warnings.warn(

[0;32m[VAIQ_NOTE]: ResidualBlock_int.onnx is generated.(quantize_result/ResidualBlock_int.onnx)[0m
-------- End of ResidualBlock-in_32x56x56-out_32-k_7-e_6-s_1-act_relu-use_se_False.pth test 
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: null
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B2304_0101000016010405
[UNILOG][INFO] Graph name: ResidualBlock, with op num: 24
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/ResidualBlock-in_32x56x56-out_32-k_7-e_6-s_1-act_relu-use_se_False.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is cd5d53847619ad371e7839bee37eb318, and has been saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************



---------------------------Done Block 12-------------------------


-----------------------Strating Qt ResidualBlock-in_32 56 56-out_48-k_3-e_6-s_2-act_relu-use_se_True.pth -----------------------------

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_32x56x56-out_48-k_3-e_6-s_2-act_relu-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization calibration process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/16 [00:00<?, ?it/s]                                                  | 0/16 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–                                              | 1/16 [00:00<00:00, 9320.68it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 2/16 [00:00<00:00, 3241.35it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 3/16 [00:00<00:00, 3242.18it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/ReLU[act]/ret.7, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 4/16 [00:00<00:00, 3394.14it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                  | 5/16 [00:00<00:00, 3486.54it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                               | 6/16 [00:00<00:00, 3110.73it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/ReLU[act]/ret.13, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 7/16 [00:00<00:00, 3373.95it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1195, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 8/16 [00:00<00:00, 3386.60it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 9/16 [00:00<00:00, 3441.09it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 10/16 [00:00<00:00, 3636.47it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 11/16 [00:00<00:00, 3676.58it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 12/16 [00:00<00:00, 3700.58it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 13/16 [00:00<00:00, 3570.79it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 14/16 [00:00<00:00, 3587.72it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret, type = batch_norm]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 15/16 [00:00<00:00, 3639.63it/s, OpInfo: name = return_0, type = Return]                                                                        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 3790.82it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 13
  0%|          | 0/13 [00:00<?, ?it/s]/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/torchquantizer.py:223: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  bnfp[1] = stats.mode(data)[0][0]
  8%|â–Š         | 1/13 [00:00<00:07,  1.51it/s] 15%|â–ˆâ–Œ        | 2/13 [00:01<00:07,  1.52it/s] 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:01<00:06,  1.51it/s] 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:02<00:05,  1.52it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:03<00:05,  1.51it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:03<00:04,  1.52it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:04<00:03,  1.52it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:05<00:03,  1.55it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:05<00:02,  1.56it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:06<00:01,  1.56it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:07<00:01,  1.57it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:07<00:00,  1.58it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:08<00:00,  1.86it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:08<00:00,  1.61it/s]

[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quantized/quant_info.json)[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0
-------- End of ResidualBlock-in_32x56x56-out_48-k_3-e_6-s_2-act_relu-use_se_True.pth test 
Compilation

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_32x56x56-out_48-k_3-e_6-s_2-act_relu-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization test process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/16 [00:00<?, ?it/s]                                                  | 0/16 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–                                              | 1/16 [00:00<00:00, 9238.56it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 2/16 [00:00<00:00, 3048.19it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 3/16 [00:00<00:00, 3050.40it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/ReLU[act]/ret.7, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 4/16 [00:00<00:00, 3265.32it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                  | 5/16 [00:00<00:00, 3359.21it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                               | 6/16 [00:00<00:00, 2997.00it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/ReLU[act]/ret.13, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 7/16 [00:00<00:00, 3268.77it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1195, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 8/16 [00:00<00:00, 3310.75it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 9/16 [00:00<00:00, 3356.04it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 10/16 [00:00<00:00, 3552.69it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 11/16 [00:00<00:00, 3596.05it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 12/16 [00:00<00:00, 3625.42it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 13/16 [00:00<00:00, 3507.39it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 14/16 [00:00<00:00, 3541.63it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret, type = batch_norm]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 15/16 [00:00<00:00, 3595.94it/s, OpInfo: name = return_0, type = Return]                                                                        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 3751.61it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 1
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 47.51it/s]

[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0

[0;32m[VAIQ_NOTE]: =>Dumping 'ResidualBlock'' checking data...[0m

[0;32m[VAIQ_NOTE]: =>Finish dumping data.(quantize_result/deploy_check_data_int/ResidualBlock)[0m

[0;32m[VAIQ_NOTE]: =>Successfully convert 'ResidualBlock' to xmodel.(quantize_result/ResidualBlock_int.xmodel)[0m
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/quantizerimpl.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if inf.sum() > 0 or nan.sum() > 0:
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/fix_ops.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (isinstance(tensor, torch.Tensor) and
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/adaptive_avg_pool.py:41: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  input_size = [int(dim) for dim in input.shape[2:]]

[0;32m[VAIQ_NOTE]: ResidualBlock_int.pt is generated.(quantize_result/ResidualBlock_int.pt)[0m
Xmodel deployed
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input ResidualBlock::input_0
  warnings.warn(

[0;32m[VAIQ_NOTE]: ResidualBlock_int.onnx is generated.(quantize_result/ResidualBlock_int.onnx)[0m
-------- End of ResidualBlock-in_32x56x56-out_48-k_3-e_6-s_2-act_relu-use_se_True.pth test 
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: null
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B2304_0101000016010405
[UNILOG][INFO] Graph name: ResidualBlock, with op num: 45
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/ResidualBlock-in_32x56x56-out_48-k_3-e_6-s_2-act_relu-use_se_True.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is c8e80e2ead373da1666343ce213723c1, and has been saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************



---------------------------Done Block 13-------------------------


-----------------------Strating Qt ResidualBlock-in_32 56 56-out_48-k_5-e_6-s_2-act_relu-use_se_True.pth -----------------------------

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_32x56x56-out_48-k_5-e_6-s_2-act_relu-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization calibration process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/16 [00:00<?, ?it/s]                                                  | 0/16 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–                                              | 1/16 [00:00<00:00, 9118.05it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 2/16 [00:00<00:00, 3188.37it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 3/16 [00:00<00:00, 3155.98it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/ReLU[act]/ret.7, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 4/16 [00:00<00:00, 3305.86it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                  | 5/16 [00:00<00:00, 3416.11it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                               | 6/16 [00:00<00:00, 3046.34it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/ReLU[act]/ret.13, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 7/16 [00:00<00:00, 3308.56it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1195, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 8/16 [00:00<00:00, 3210.95it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 9/16 [00:00<00:00, 3223.36it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 10/16 [00:00<00:00, 3404.47it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 11/16 [00:00<00:00, 3449.52it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 12/16 [00:00<00:00, 3461.12it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 13/16 [00:00<00:00, 3349.47it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 14/16 [00:00<00:00, 3380.36it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret, type = batch_norm]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 15/16 [00:00<00:00, 3439.27it/s, OpInfo: name = return_0, type = Return]                                                                        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 3593.32it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 13
  0%|          | 0/13 [00:00<?, ?it/s]/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/torchquantizer.py:223: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  bnfp[1] = stats.mode(data)[0][0]
  8%|â–Š         | 1/13 [00:01<00:15,  1.30s/it] 15%|â–ˆâ–Œ        | 2/13 [00:02<00:12,  1.18s/it] 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:03<00:09,  1.05it/s] 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:03<00:07,  1.18it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:04<00:06,  1.28it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:05<00:05,  1.33it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:05<00:04,  1.37it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:06<00:03,  1.40it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:07<00:02,  1.44it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:07<00:02,  1.47it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:08<00:01,  1.50it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:09<00:00,  1.51it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:09<00:00,  1.79it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:09<00:00,  1.38it/s]

[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quantized/quant_info.json)[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0
-------- End of ResidualBlock-in_32x56x56-out_48-k_5-e_6-s_2-act_relu-use_se_True.pth test 
Compilation

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_32x56x56-out_48-k_5-e_6-s_2-act_relu-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization test process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/16 [00:00<?, ?it/s]                                                  | 0/16 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–                                              | 1/16 [00:00<00:00, 9799.78it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 2/16 [00:00<00:00, 3206.65it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 3/16 [00:00<00:00, 3175.90it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/ReLU[act]/ret.7, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 4/16 [00:00<00:00, 3367.57it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                  | 5/16 [00:00<00:00, 3462.93it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                               | 6/16 [00:00<00:00, 3050.03it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/ReLU[act]/ret.13, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 7/16 [00:00<00:00, 3287.07it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1195, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 8/16 [00:00<00:00, 3298.71it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 9/16 [00:00<00:00, 3348.60it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 10/16 [00:00<00:00, 3536.81it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 11/16 [00:00<00:00, 3579.04it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 12/16 [00:00<00:00, 3593.32it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 13/16 [00:00<00:00, 3457.36it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 14/16 [00:00<00:00, 3488.61it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret, type = batch_norm]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 15/16 [00:00<00:00, 3540.69it/s, OpInfo: name = return_0, type = Return]                                                                        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 3698.07it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 1
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 46.46it/s]

[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0

[0;32m[VAIQ_NOTE]: =>Dumping 'ResidualBlock'' checking data...[0m

[0;32m[VAIQ_NOTE]: =>Finish dumping data.(quantize_result/deploy_check_data_int/ResidualBlock)[0m

[0;32m[VAIQ_NOTE]: =>Successfully convert 'ResidualBlock' to xmodel.(quantize_result/ResidualBlock_int.xmodel)[0m
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/quantizerimpl.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if inf.sum() > 0 or nan.sum() > 0:
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/fix_ops.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (isinstance(tensor, torch.Tensor) and
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/adaptive_avg_pool.py:41: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  input_size = [int(dim) for dim in input.shape[2:]]

[0;32m[VAIQ_NOTE]: ResidualBlock_int.pt is generated.(quantize_result/ResidualBlock_int.pt)[0m
Xmodel deployed
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input ResidualBlock::input_0
  warnings.warn(

[0;32m[VAIQ_NOTE]: ResidualBlock_int.onnx is generated.(quantize_result/ResidualBlock_int.onnx)[0m
-------- End of ResidualBlock-in_32x56x56-out_48-k_5-e_6-s_2-act_relu-use_se_True.pth test 
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: null
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B2304_0101000016010405
[UNILOG][INFO] Graph name: ResidualBlock, with op num: 45
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/ResidualBlock-in_32x56x56-out_48-k_5-e_6-s_2-act_relu-use_se_True.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 71b8176068d29cd8ed12ff786713277b, and has been saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************



---------------------------Done Block 14-------------------------


-----------------------Strating Qt ResidualBlock-in_32 56 56-out_48-k_7-e_6-s_2-act_relu-use_se_True.pth -----------------------------

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_32x56x56-out_48-k_7-e_6-s_2-act_relu-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization calibration process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/16 [00:00<?, ?it/s]                                                  | 0/16 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–                                              | 1/16 [00:00<00:00, 9362.29it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 2/16 [00:00<00:00, 3255.18it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 3/16 [00:00<00:00, 3254.76it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/ReLU[act]/ret.7, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 4/16 [00:00<00:00, 3401.71it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                  | 5/16 [00:00<00:00, 3522.85it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                               | 6/16 [00:00<00:00, 3129.69it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/ReLU[act]/ret.13, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 7/16 [00:00<00:00, 3398.16it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1195, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 8/16 [00:00<00:00, 3434.79it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 9/16 [00:00<00:00, 3475.62it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 10/16 [00:00<00:00, 3669.88it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 11/16 [00:00<00:00, 3708.79it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 12/16 [00:00<00:00, 3730.76it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 13/16 [00:00<00:00, 3594.33it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 14/16 [00:00<00:00, 3617.12it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret, type = batch_norm]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 15/16 [00:00<00:00, 3658.25it/s, OpInfo: name = return_0, type = Return]                                                                        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 3816.69it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 13
  0%|          | 0/13 [00:00<?, ?it/s]/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/torchquantizer.py:223: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  bnfp[1] = stats.mode(data)[0][0]
  8%|â–Š         | 1/13 [00:00<00:08,  1.46it/s] 15%|â–ˆâ–Œ        | 2/13 [00:01<00:07,  1.47it/s] 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:02<00:06,  1.47it/s] 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:02<00:06,  1.47it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:03<00:05,  1.47it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:04<00:04,  1.48it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:04<00:04,  1.48it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:05<00:03,  1.50it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:06<00:02,  1.52it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:06<00:01,  1.52it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:07<00:01,  1.52it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:07<00:00,  1.53it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:08<00:00,  1.80it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:08<00:00,  1.56it/s]

[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quantized/quant_info.json)[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0
-------- End of ResidualBlock-in_32x56x56-out_48-k_7-e_6-s_2-act_relu-use_se_True.pth test 
Compilation

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_32x56x56-out_48-k_7-e_6-s_2-act_relu-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization test process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/16 [00:00<?, ?it/s]                                                  | 0/16 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–                                              | 1/16 [00:00<00:00, 9597.95it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 2/16 [00:00<00:00, 3315.66it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 3/16 [00:00<00:00, 3243.02it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/ReLU[act]/ret.7, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 4/16 [00:00<00:00, 3412.78it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                  | 5/16 [00:00<00:00, 3446.43it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                               | 6/16 [00:00<00:00, 3032.03it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/ReLU[act]/ret.13, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 7/16 [00:00<00:00, 3282.66it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1195, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 8/16 [00:00<00:00, 3325.18it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 9/16 [00:00<00:00, 3386.14it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 10/16 [00:00<00:00, 3580.90it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 11/16 [00:00<00:00, 3628.58it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 12/16 [00:00<00:00, 3650.13it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 13/16 [00:00<00:00, 3510.33it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 14/16 [00:00<00:00, 3519.56it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret, type = batch_norm]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 15/16 [00:00<00:00, 3572.66it/s, OpInfo: name = return_0, type = Return]                                                                        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 3729.93it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 1
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 45.94it/s]

[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0

[0;32m[VAIQ_NOTE]: =>Dumping 'ResidualBlock'' checking data...[0m

[0;32m[VAIQ_NOTE]: =>Finish dumping data.(quantize_result/deploy_check_data_int/ResidualBlock)[0m

[0;32m[VAIQ_NOTE]: =>Successfully convert 'ResidualBlock' to xmodel.(quantize_result/ResidualBlock_int.xmodel)[0m
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/quantizerimpl.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if inf.sum() > 0 or nan.sum() > 0:
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/fix_ops.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (isinstance(tensor, torch.Tensor) and
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/adaptive_avg_pool.py:41: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  input_size = [int(dim) for dim in input.shape[2:]]

[0;32m[VAIQ_NOTE]: ResidualBlock_int.pt is generated.(quantize_result/ResidualBlock_int.pt)[0m
Xmodel deployed
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input ResidualBlock::input_0
  warnings.warn(

[0;32m[VAIQ_NOTE]: ResidualBlock_int.onnx is generated.(quantize_result/ResidualBlock_int.onnx)[0m
-------- End of ResidualBlock-in_32x56x56-out_48-k_7-e_6-s_2-act_relu-use_se_True.pth test 
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: null
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B2304_0101000016010405
[UNILOG][INFO] Graph name: ResidualBlock, with op num: 45
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/ResidualBlock-in_32x56x56-out_48-k_7-e_6-s_2-act_relu-use_se_True.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 39a9885681feaacd54263d46556cd1dd, and has been saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************



---------------------------Done Block 15-------------------------


-----------------------Strating Qt ResidualBlock-in_48 28 28-out_48-k_3-e_6-s_1-act_relu-use_se_True.pth -----------------------------

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_48x28x28-out_48-k_3-e_6-s_1-act_relu-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization calibration process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/17 [00:00<?, ?it/s]                                                  | 0/17 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–‰                                               | 1/17 [00:00<00:00, 9078.58it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                            | 2/17 [00:00<00:00, 3207.88it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 3/17 [00:00<00:00, 3130.86it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/ReLU[act]/ret.7, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                      | 4/17 [00:00<00:00, 3282.57it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 5/17 [00:00<00:00, 2837.82it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 6/17 [00:00<00:00, 2972.22it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/ReLU[act]/ret.13, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 7/17 [00:00<00:00, 3260.43it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1205, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 8/17 [00:00<00:00, 3279.36it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 9/17 [00:00<00:00, 3329.11it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 10/17 [00:00<00:00, 3517.83it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 | 11/17 [00:00<00:00, 3556.96it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 12/17 [00:00<00:00, 3565.07it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 13/17 [00:00<00:00, 3438.82it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 14/17 [00:00<00:00, 3472.11it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.27, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 15/17 [00:00<00:00, 3523.24it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16/17 [00:00<00:00, 3474.26it/s, OpInfo: name = return_0, type = Return]      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 3610.84it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 13
  0%|          | 0/13 [00:00<?, ?it/s]/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/torchquantizer.py:223: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  bnfp[1] = stats.mode(data)[0][0]
  8%|â–Š         | 1/13 [00:00<00:06,  1.89it/s] 15%|â–ˆâ–Œ        | 2/13 [00:01<00:05,  1.92it/s] 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:01<00:05,  1.95it/s] 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:02<00:04,  1.98it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:02<00:04,  1.99it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:04<00:05,  1.17it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:04<00:04,  1.32it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:05<00:03,  1.48it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:05<00:02,  1.62it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:06<00:01,  1.72it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:06<00:01,  1.80it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:07<00:00,  1.87it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:07<00:00,  2.24it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:07<00:00,  1.77it/s]

[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quantized/quant_info.json)[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0
-------- End of ResidualBlock-in_48x28x28-out_48-k_3-e_6-s_1-act_relu-use_se_True.pth test 
Compilation

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_48x28x28-out_48-k_3-e_6-s_1-act_relu-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization test process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/17 [00:00<?, ?it/s]                                                  | 0/17 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–‰                                               | 1/17 [00:00<00:00, 9000.65it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                            | 2/17 [00:00<00:00, 3235.10it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 3/17 [00:00<00:00, 3163.92it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/ReLU[act]/ret.7, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                      | 4/17 [00:00<00:00, 3363.52it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 5/17 [00:00<00:00, 2927.35it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 6/17 [00:00<00:00, 3058.56it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/ReLU[act]/ret.13, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 7/17 [00:00<00:00, 3349.32it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1205, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 8/17 [00:00<00:00, 3367.90it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 9/17 [00:00<00:00, 3424.85it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 10/17 [00:00<00:00, 3622.34it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 | 11/17 [00:00<00:00, 3656.47it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 12/17 [00:00<00:00, 3658.89it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 13/17 [00:00<00:00, 3524.40it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 14/17 [00:00<00:00, 3548.27it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.27, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 15/17 [00:00<00:00, 3551.28it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16/17 [00:00<00:00, 3484.36it/s, OpInfo: name = return_0, type = Return]      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 3625.71it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 1
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 58.67it/s]

[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0

[0;32m[VAIQ_NOTE]: =>Dumping 'ResidualBlock'' checking data...[0m

[0;32m[VAIQ_NOTE]: =>Finish dumping data.(quantize_result/deploy_check_data_int/ResidualBlock)[0m

[0;32m[VAIQ_NOTE]: =>Successfully convert 'ResidualBlock' to xmodel.(quantize_result/ResidualBlock_int.xmodel)[0m
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/quantizerimpl.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if inf.sum() > 0 or nan.sum() > 0:
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/fix_ops.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (isinstance(tensor, torch.Tensor) and
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/adaptive_avg_pool.py:41: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  input_size = [int(dim) for dim in input.shape[2:]]

[0;32m[VAIQ_NOTE]: ResidualBlock_int.pt is generated.(quantize_result/ResidualBlock_int.pt)[0m
Xmodel deployed
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input ResidualBlock::input_0
  warnings.warn(

[0;32m[VAIQ_NOTE]: ResidualBlock_int.onnx is generated.(quantize_result/ResidualBlock_int.onnx)[0m
-------- End of ResidualBlock-in_48x28x28-out_48-k_3-e_6-s_1-act_relu-use_se_True.pth test 
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: null
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B2304_0101000016010405
[UNILOG][INFO] Graph name: ResidualBlock, with op num: 47
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/ResidualBlock-in_48x28x28-out_48-k_3-e_6-s_1-act_relu-use_se_True.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is d3fca2f93ec72ff13d664a6f36bb53dc, and has been saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************



---------------------------Done Block 16-------------------------


-----------------------Strating Qt ResidualBlock-in_48 28 28-out_48-k_5-e_6-s_1-act_relu-use_se_True.pth -----------------------------

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_48x28x28-out_48-k_5-e_6-s_1-act_relu-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization calibration process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/17 [00:00<?, ?it/s]                                                  | 0/17 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–‰                                               | 1/17 [00:00<00:00, 9619.96it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                            | 2/17 [00:00<00:00, 3378.42it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 3/17 [00:00<00:00, 3219.78it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/ReLU[act]/ret.7, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                      | 4/17 [00:00<00:00, 3397.57it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 5/17 [00:00<00:00, 2940.89it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 6/17 [00:00<00:00, 3088.59it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/ReLU[act]/ret.13, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 7/17 [00:00<00:00, 3367.37it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1205, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 8/17 [00:00<00:00, 3381.14it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 9/17 [00:00<00:00, 3419.27it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 10/17 [00:00<00:00, 3608.62it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 | 11/17 [00:00<00:00, 3634.01it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 12/17 [00:00<00:00, 3650.13it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 13/17 [00:00<00:00, 3520.07it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 14/17 [00:00<00:00, 3551.70it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.27, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 15/17 [00:00<00:00, 3587.94it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16/17 [00:00<00:00, 3528.70it/s, OpInfo: name = return_0, type = Return]      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 3672.01it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 13
  0%|          | 0/13 [00:00<?, ?it/s]/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/torchquantizer.py:223: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  bnfp[1] = stats.mode(data)[0][0]
  8%|â–Š         | 1/13 [00:00<00:06,  1.89it/s] 15%|â–ˆâ–Œ        | 2/13 [00:01<00:05,  1.95it/s] 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:01<00:05,  1.98it/s] 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:02<00:04,  1.96it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:02<00:04,  1.97it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:03<00:03,  1.98it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:03<00:03,  1.99it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:04<00:02,  1.98it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:04<00:02,  2.00it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:05<00:01,  1.99it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:05<00:01,  1.99it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:06<00:00,  2.00it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:06<00:00,  2.36it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:06<00:00,  2.06it/s]

[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quantized/quant_info.json)[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0
-------- End of ResidualBlock-in_48x28x28-out_48-k_5-e_6-s_1-act_relu-use_se_True.pth test 
Compilation

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_48x28x28-out_48-k_5-e_6-s_1-act_relu-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization test process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/17 [00:00<?, ?it/s]                                                  | 0/17 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–‰                                               | 1/17 [00:00<00:00, 9489.38it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                            | 2/17 [00:00<00:00, 3314.35it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 3/17 [00:00<00:00, 3272.54it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/ReLU[act]/ret.7, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                      | 4/17 [00:00<00:00, 3496.71it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 5/17 [00:00<00:00, 3026.19it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 6/17 [00:00<00:00, 3179.51it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/ReLU[act]/ret.13, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 7/17 [00:00<00:00, 3477.04it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1205, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 8/17 [00:00<00:00, 3506.21it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 9/17 [00:00<00:00, 3553.49it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 10/17 [00:00<00:00, 3747.93it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 | 11/17 [00:00<00:00, 3794.19it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 12/17 [00:00<00:00, 3808.10it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 13/17 [00:00<00:00, 3626.12it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 14/17 [00:00<00:00, 3643.60it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.27, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 15/17 [00:00<00:00, 3656.97it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16/17 [00:00<00:00, 3601.81it/s, OpInfo: name = return_0, type = Return]      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 3748.06it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 1
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.00it/s]

[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0

[0;32m[VAIQ_NOTE]: =>Dumping 'ResidualBlock'' checking data...[0m

[0;32m[VAIQ_NOTE]: =>Finish dumping data.(quantize_result/deploy_check_data_int/ResidualBlock)[0m

[0;32m[VAIQ_NOTE]: =>Successfully convert 'ResidualBlock' to xmodel.(quantize_result/ResidualBlock_int.xmodel)[0m
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/quantizerimpl.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if inf.sum() > 0 or nan.sum() > 0:
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/fix_ops.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (isinstance(tensor, torch.Tensor) and
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/adaptive_avg_pool.py:41: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  input_size = [int(dim) for dim in input.shape[2:]]

[0;32m[VAIQ_NOTE]: ResidualBlock_int.pt is generated.(quantize_result/ResidualBlock_int.pt)[0m
Xmodel deployed
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input ResidualBlock::input_0
  warnings.warn(

[0;32m[VAIQ_NOTE]: ResidualBlock_int.onnx is generated.(quantize_result/ResidualBlock_int.onnx)[0m
-------- End of ResidualBlock-in_48x28x28-out_48-k_5-e_6-s_1-act_relu-use_se_True.pth test 
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: null
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B2304_0101000016010405
[UNILOG][INFO] Graph name: ResidualBlock, with op num: 47
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/ResidualBlock-in_48x28x28-out_48-k_5-e_6-s_1-act_relu-use_se_True.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is e0305739515b605b867a887f6bdf7ddc, and has been saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************



---------------------------Done Block 17-------------------------


-----------------------Strating Qt ResidualBlock-in_48 28 28-out_48-k_7-e_6-s_1-act_relu-use_se_True.pth -----------------------------

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_48x28x28-out_48-k_7-e_6-s_1-act_relu-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization calibration process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/17 [00:00<?, ?it/s]                                                  | 0/17 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–‰                                               | 1/17 [00:00<00:00, 6114.15it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                            | 2/17 [00:00<00:00, 2223.92it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 3/17 [00:00<00:00, 2321.57it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/ReLU[act]/ret.7, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                      | 4/17 [00:00<00:00, 2529.36it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 5/17 [00:00<00:00, 2200.58it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 6/17 [00:00<00:00, 2358.34it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/ReLU[act]/ret.13, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 7/17 [00:00<00:00, 2613.97it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1205, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 8/17 [00:00<00:00, 2597.70it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 9/17 [00:00<00:00, 2608.76it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 10/17 [00:00<00:00, 2770.16it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 | 11/17 [00:00<00:00, 2845.70it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 12/17 [00:00<00:00, 2863.01it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 13/17 [00:00<00:00, 2765.01it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 14/17 [00:00<00:00, 2793.54it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.27, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 15/17 [00:00<00:00, 2857.29it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16/17 [00:00<00:00, 2849.63it/s, OpInfo: name = return_0, type = Return]      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 2974.68it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 13
  0%|          | 0/13 [00:00<?, ?it/s]/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/torchquantizer.py:223: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  bnfp[1] = stats.mode(data)[0][0]
  8%|â–Š         | 1/13 [00:00<00:07,  1.54it/s] 15%|â–ˆâ–Œ        | 2/13 [00:01<00:07,  1.51it/s] 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:01<00:06,  1.50it/s] 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:02<00:05,  1.53it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:03<00:05,  1.49it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:04<00:04,  1.49it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:04<00:03,  1.57it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:05<00:03,  1.65it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:06<00:03,  1.16it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:07<00:02,  1.31it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:07<00:01,  1.41it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:08<00:00,  1.52it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:08<00:00,  1.82it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:08<00:00,  1.53it/s]

[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quantized/quant_info.json)[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0
-------- End of ResidualBlock-in_48x28x28-out_48-k_7-e_6-s_1-act_relu-use_se_True.pth test 
Compilation

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_48x28x28-out_48-k_7-e_6-s_1-act_relu-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization test process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/17 [00:00<?, ?it/s]                                                  | 0/17 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–‰                                               | 1/17 [00:00<00:00, 6087.52it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                            | 2/17 [00:00<00:00, 1916.08it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                         | 3/17 [00:00<00:00, 1731.04it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/ReLU[act]/ret.7, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                      | 4/17 [00:00<00:00, 1673.20it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 5/17 [00:00<00:00, 1529.43it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 6/17 [00:00<00:00, 1664.85it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/ReLU[act]/ret.13, type = relu]            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 7/17 [00:00<00:00, 1866.03it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1205, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 8/17 [00:00<00:00, 1936.20it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 9/17 [00:00<00:00, 1987.19it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 10/17 [00:00<00:00, 2088.38it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 | 11/17 [00:00<00:00, 2141.34it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 12/17 [00:00<00:00, 2129.81it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 13/17 [00:00<00:00, 2026.23it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 14/17 [00:00<00:00, 2068.27it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.27, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 15/17 [00:00<00:00, 2133.20it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16/17 [00:00<00:00, 2144.88it/s, OpInfo: name = return_0, type = Return]      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 2249.81it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 1
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 16.99it/s]

[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0

[0;32m[VAIQ_NOTE]: =>Dumping 'ResidualBlock'' checking data...[0m

[0;32m[VAIQ_NOTE]: =>Finish dumping data.(quantize_result/deploy_check_data_int/ResidualBlock)[0m

[0;32m[VAIQ_NOTE]: =>Successfully convert 'ResidualBlock' to xmodel.(quantize_result/ResidualBlock_int.xmodel)[0m
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/quantizerimpl.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if inf.sum() > 0 or nan.sum() > 0:
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/fix_ops.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (isinstance(tensor, torch.Tensor) and
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/adaptive_avg_pool.py:41: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  input_size = [int(dim) for dim in input.shape[2:]]

[0;32m[VAIQ_NOTE]: ResidualBlock_int.pt is generated.(quantize_result/ResidualBlock_int.pt)[0m
Xmodel deployed
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input ResidualBlock::input_0
  warnings.warn(

[0;32m[VAIQ_NOTE]: ResidualBlock_int.onnx is generated.(quantize_result/ResidualBlock_int.onnx)[0m
-------- End of ResidualBlock-in_48x28x28-out_48-k_7-e_6-s_1-act_relu-use_se_True.pth test 
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: null
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B2304_0101000016010405
[UNILOG][INFO] Graph name: ResidualBlock, with op num: 47
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/ResidualBlock-in_48x28x28-out_48-k_7-e_6-s_1-act_relu-use_se_True.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 1cf5d12da425e1ff2841ca53a3b54d2b, and has been saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************



---------------------------Done Block 18-------------------------


-----------------------Strating Qt ResidualBlock-in_48 28 28-out_96-k_3-e_6-s_2-act_h_swish-use_se_False.pth -----------------------------

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_48x28x28-out_96-k_3-e_6-s_2-act_h_swish-use_se_False.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization calibration process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/10 [00:00<?, ?it/s]                                                  | 0/10 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                             | 1/10 [00:00<00:00, 9532.51it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 2/10 [00:00<00:00, 3352.76it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 3/10 [00:00<00:00, 3232.19it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 4/10 [00:00<00:00, 3365.54it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 5/10 [00:00<00:00, 3492.92it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 6/10 [00:00<00:00, 3602.32it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 7/10 [00:00<00:00, 3845.47it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.15, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 8/10 [00:00<00:00, 3918.08it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret, type = batch_norm]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/10 [00:00<00:00, 4011.56it/s, OpInfo: name = return_0, type = Return]                                                                        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 4293.48it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 13
  0%|          | 0/13 [00:00<?, ?it/s]/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/torchquantizer.py:223: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  bnfp[1] = stats.mode(data)[0][0]
  8%|â–Š         | 1/13 [00:00<00:06,  1.75it/s] 15%|â–ˆâ–Œ        | 2/13 [00:01<00:06,  1.73it/s] 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:01<00:05,  1.76it/s] 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:02<00:05,  1.78it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:02<00:04,  1.76it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:03<00:03,  1.78it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:03<00:03,  1.80it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:04<00:02,  1.80it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:05<00:02,  1.79it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:05<00:01,  1.80it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:06<00:01,  1.78it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:06<00:00,  1.82it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:07<00:00,  2.10it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:07<00:00,  1.86it/s]

[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quantized/quant_info.json)[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0
-------- End of ResidualBlock-in_48x28x28-out_96-k_3-e_6-s_2-act_h_swish-use_se_False.pth test 
Compilation

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_48x28x28-out_96-k_3-e_6-s_2-act_h_swish-use_se_False.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization test process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/10 [00:00<?, ?it/s]                                                  | 0/10 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                             | 1/10 [00:00<00:00, 7084.97it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 2/10 [00:00<00:00, 1931.08it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 3/10 [00:00<00:00, 1687.17it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 4/10 [00:00<00:00, 1559.08it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 5/10 [00:00<00:00, 1569.02it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 6/10 [00:00<00:00, 1634.68it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 7/10 [00:00<00:00, 1763.37it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.15, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 8/10 [00:00<00:00, 1854.55it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret, type = batch_norm]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/10 [00:00<00:00, 1941.21it/s, OpInfo: name = return_0, type = Return]                                                                        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 2084.02it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 1
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 17.16it/s]

[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0

[0;32m[VAIQ_NOTE]: =>Dumping 'ResidualBlock'' checking data...[0m

[0;32m[VAIQ_NOTE]: =>Finish dumping data.(quantize_result/deploy_check_data_int/ResidualBlock)[0m

[0;32m[VAIQ_NOTE]: =>Successfully convert 'ResidualBlock' to xmodel.(quantize_result/ResidualBlock_int.xmodel)[0m
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/quantizerimpl.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if inf.sum() > 0 or nan.sum() > 0:
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/fix_ops.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (isinstance(tensor, torch.Tensor) and

[0;32m[VAIQ_NOTE]: ResidualBlock_int.pt is generated.(quantize_result/ResidualBlock_int.pt)[0m
Xmodel deployed
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input ResidualBlock::input_0
  warnings.warn(

[0;32m[VAIQ_NOTE]: ResidualBlock_int.onnx is generated.(quantize_result/ResidualBlock_int.onnx)[0m
-------- End of ResidualBlock-in_48x28x28-out_96-k_3-e_6-s_2-act_h_swish-use_se_False.pth test 
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: null
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B2304_0101000016010405
[UNILOG][INFO] Graph name: ResidualBlock, with op num: 32
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/ResidualBlock-in_48x28x28-out_96-k_3-e_6-s_2-act_h_swish-use_se_False.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is b59dfa82fbfc0b2cbc848b87d7a33a22, and has been saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************



---------------------------Done Block 19-------------------------


-----------------------Strating Qt ResidualBlock-in_48 28 28-out_96-k_5-e_6-s_2-act_h_swish-use_se_False.pth -----------------------------

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_48x28x28-out_96-k_5-e_6-s_2-act_h_swish-use_se_False.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization calibration process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/10 [00:00<?, ?it/s]                                                  | 0/10 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                             | 1/10 [00:00<00:00, 10106.76it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 2/10 [00:00<00:00, 3350.08it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 3/10 [00:00<00:00, 3207.47it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 4/10 [00:00<00:00, 3234.47it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 5/10 [00:00<00:00, 2773.28it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 6/10 [00:00<00:00, 2728.89it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 7/10 [00:00<00:00, 2831.80it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.15, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 8/10 [00:00<00:00, 2884.42it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret, type = batch_norm]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/10 [00:00<00:00, 2948.66it/s, OpInfo: name = return_0, type = Return]                                                                        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 3135.46it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 13
  0%|          | 0/13 [00:00<?, ?it/s]/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/torchquantizer.py:223: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  bnfp[1] = stats.mode(data)[0][0]
  8%|â–Š         | 1/13 [00:00<00:07,  1.71it/s] 15%|â–ˆâ–Œ        | 2/13 [00:01<00:06,  1.69it/s] 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:01<00:05,  1.73it/s] 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:02<00:05,  1.73it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:03<00:07,  1.13it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:04<00:05,  1.29it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:04<00:04,  1.43it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:05<00:03,  1.53it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:05<00:02,  1.60it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:06<00:01,  1.57it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:07<00:01,  1.63it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:07<00:00,  1.65it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:08<00:00,  1.95it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:08<00:00,  1.61it/s]

[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quantized/quant_info.json)[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0
-------- End of ResidualBlock-in_48x28x28-out_96-k_5-e_6-s_2-act_h_swish-use_se_False.pth test 
Compilation

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_48x28x28-out_96-k_5-e_6-s_2-act_h_swish-use_se_False.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization test process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/10 [00:00<?, ?it/s]                                                  | 0/10 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                             | 1/10 [00:00<00:00, 10155.70it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 2/10 [00:00<00:00, 3411.39it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 3/10 [00:00<00:00, 3377.06it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 4/10 [00:00<00:00, 3517.24it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 5/10 [00:00<00:00, 3630.17it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 6/10 [00:00<00:00, 3767.34it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 7/10 [00:00<00:00, 4032.98it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.15, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 8/10 [00:00<00:00, 4093.00it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret, type = batch_norm]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/10 [00:00<00:00, 4178.52it/s, OpInfo: name = return_0, type = Return]                                                                        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 4465.35it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 1
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 18.35it/s]

[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0

[0;32m[VAIQ_NOTE]: =>Dumping 'ResidualBlock'' checking data...[0m

[0;32m[VAIQ_NOTE]: =>Finish dumping data.(quantize_result/deploy_check_data_int/ResidualBlock)[0m

[0;32m[VAIQ_NOTE]: =>Successfully convert 'ResidualBlock' to xmodel.(quantize_result/ResidualBlock_int.xmodel)[0m
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/quantizerimpl.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if inf.sum() > 0 or nan.sum() > 0:
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/fix_ops.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (isinstance(tensor, torch.Tensor) and

[0;32m[VAIQ_NOTE]: ResidualBlock_int.pt is generated.(quantize_result/ResidualBlock_int.pt)[0m
Xmodel deployed
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input ResidualBlock::input_0
  warnings.warn(

[0;32m[VAIQ_NOTE]: ResidualBlock_int.onnx is generated.(quantize_result/ResidualBlock_int.onnx)[0m
-------- End of ResidualBlock-in_48x28x28-out_96-k_5-e_6-s_2-act_h_swish-use_se_False.pth test 
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: null
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B2304_0101000016010405
[UNILOG][INFO] Graph name: ResidualBlock, with op num: 32
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/ResidualBlock-in_48x28x28-out_96-k_5-e_6-s_2-act_h_swish-use_se_False.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is f64fad1a4f52f36adadd9b02129cc708, and has been saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************



---------------------------Done Block 20-------------------------


-----------------------Strating Qt ResidualBlock-in_48 28 28-out_96-k_7-e_6-s_2-act_h_swish-use_se_False.pth -----------------------------

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_48x28x28-out_96-k_7-e_6-s_2-act_h_swish-use_se_False.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization calibration process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/10 [00:00<?, ?it/s]                                                  | 0/10 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                             | 1/10 [00:00<00:00, 6743.25it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 2/10 [00:00<00:00, 2092.97it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 3/10 [00:00<00:00, 1812.84it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 4/10 [00:00<00:00, 1824.60it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 5/10 [00:00<00:00, 2015.91it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 6/10 [00:00<00:00, 2216.47it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 7/10 [00:00<00:00, 2460.00it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.15, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 8/10 [00:00<00:00, 2615.31it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret, type = batch_norm]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/10 [00:00<00:00, 2764.87it/s, OpInfo: name = return_0, type = Return]                                                                        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 2990.59it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 13
  0%|          | 0/13 [00:00<?, ?it/s]/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/torchquantizer.py:223: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  bnfp[1] = stats.mode(data)[0][0]
  8%|â–Š         | 1/13 [00:00<00:07,  1.69it/s] 15%|â–ˆâ–Œ        | 2/13 [00:01<00:06,  1.65it/s] 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:01<00:05,  1.72it/s] 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:02<00:05,  1.75it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:02<00:04,  1.77it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:03<00:03,  1.79it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:03<00:03,  1.78it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:04<00:02,  1.78it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:05<00:02,  1.79it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:05<00:01,  1.79it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:06<00:01,  1.79it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:06<00:00,  1.80it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:07<00:00,  2.10it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:07<00:00,  1.84it/s]

[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quantized/quant_info.json)[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0
-------- End of ResidualBlock-in_48x28x28-out_96-k_7-e_6-s_2-act_h_swish-use_se_False.pth test 
Compilation

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_48x28x28-out_96-k_7-e_6-s_2-act_h_swish-use_se_False.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization test process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/10 [00:00<?, ?it/s]                                                  | 0/10 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                             | 1/10 [00:00<00:00, 6393.76it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 2/10 [00:00<00:00, 2082.06it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 3/10 [00:00<00:00, 1878.61it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 4/10 [00:00<00:00, 2000.38it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 5/10 [00:00<00:00, 2190.01it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 6/10 [00:00<00:00, 2372.57it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 7/10 [00:00<00:00, 2610.95it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.15, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 8/10 [00:00<00:00, 2745.41it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret, type = batch_norm]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/10 [00:00<00:00, 2876.53it/s, OpInfo: name = return_0, type = Return]                                                                        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 3100.46it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 1
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15.07it/s]

[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0

[0;32m[VAIQ_NOTE]: =>Dumping 'ResidualBlock'' checking data...[0m

[0;32m[VAIQ_NOTE]: =>Finish dumping data.(quantize_result/deploy_check_data_int/ResidualBlock)[0m

[0;32m[VAIQ_NOTE]: =>Successfully convert 'ResidualBlock' to xmodel.(quantize_result/ResidualBlock_int.xmodel)[0m
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/quantizerimpl.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if inf.sum() > 0 or nan.sum() > 0:
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/fix_ops.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (isinstance(tensor, torch.Tensor) and

[0;32m[VAIQ_NOTE]: ResidualBlock_int.pt is generated.(quantize_result/ResidualBlock_int.pt)[0m
Xmodel deployed
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input ResidualBlock::input_0
  warnings.warn(

[0;32m[VAIQ_NOTE]: ResidualBlock_int.onnx is generated.(quantize_result/ResidualBlock_int.onnx)[0m
-------- End of ResidualBlock-in_48x28x28-out_96-k_7-e_6-s_2-act_h_swish-use_se_False.pth test 
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: null
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B2304_0101000016010405
[UNILOG][INFO] Graph name: ResidualBlock, with op num: 32
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/ResidualBlock-in_48x28x28-out_96-k_7-e_6-s_2-act_h_swish-use_se_False.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 34732fce35a6c497f05ee2df7d6d3ccd, and has been saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************



---------------------------Done Block 21-------------------------


-----------------------Strating Qt ResidualBlock-in_96 14 14-out_136-k_3-e_4-s_1-act_h_swish-use_se_True.pth -----------------------------

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_96x14x14-out_136-k_3-e_4-s_1-act_h_swish-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization calibration process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/16 [00:00<?, ?it/s]                                                  | 0/16 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–                                              | 1/16 [00:00<00:00, 6898.53it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 2/16 [00:00<00:00, 2416.77it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 3/16 [00:00<00:00, 2526.18it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 4/16 [00:00<00:00, 2673.66it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                  | 5/16 [00:00<00:00, 2289.97it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                               | 6/16 [00:00<00:00, 2441.15it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 7/16 [00:00<00:00, 2664.98it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1195, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 8/16 [00:00<00:00, 2704.91it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 9/16 [00:00<00:00, 2789.59it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 10/16 [00:00<00:00, 2876.16it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 11/16 [00:00<00:00, 2941.87it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 12/16 [00:00<00:00, 3007.57it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 13/16 [00:00<00:00, 2890.94it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 14/16 [00:00<00:00, 2934.99it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret, type = batch_norm]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 15/16 [00:00<00:00, 3005.66it/s, OpInfo: name = return_0, type = Return]                                                                        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 3131.25it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 13
  0%|          | 0/13 [00:00<?, ?it/s]/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/torchquantizer.py:223: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  bnfp[1] = stats.mode(data)[0][0]
  8%|â–Š         | 1/13 [00:00<00:05,  2.16it/s] 15%|â–ˆâ–Œ        | 2/13 [00:00<00:04,  2.20it/s] 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:01<00:04,  2.30it/s] 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:01<00:03,  2.32it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:02<00:03,  2.34it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:02<00:02,  2.38it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:02<00:02,  2.39it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:03<00:02,  2.38it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:03<00:01,  2.39it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:04<00:01,  2.41it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:04<00:00,  2.41it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:05<00:00,  2.45it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:05<00:00,  2.78it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:05<00:00,  2.46it/s]

[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quantized/quant_info.json)[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0
-------- End of ResidualBlock-in_96x14x14-out_136-k_3-e_4-s_1-act_h_swish-use_se_True.pth test 
Compilation

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_96x14x14-out_136-k_3-e_4-s_1-act_h_swish-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization test process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/16 [00:00<?, ?it/s]                                                  | 0/16 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–                                              | 1/16 [00:00<00:00, 6017.65it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 2/16 [00:00<00:00, 1846.49it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 3/16 [00:00<00:00, 1763.79it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 4/16 [00:00<00:00, 1927.31it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                  | 5/16 [00:00<00:00, 1849.18it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                               | 6/16 [00:00<00:00, 1950.69it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 7/16 [00:00<00:00, 2132.03it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1195, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 8/16 [00:00<00:00, 2185.96it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 9/16 [00:00<00:00, 2286.83it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 10/16 [00:00<00:00, 2393.46it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 11/16 [00:00<00:00, 2476.38it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 12/16 [00:00<00:00, 2556.85it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 13/16 [00:00<00:00, 2515.50it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 14/16 [00:00<00:00, 2586.11it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret, type = batch_norm]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 15/16 [00:00<00:00, 2638.37it/s, OpInfo: name = return_0, type = Return]                                                                        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 2764.64it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 1
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 16.06it/s]

[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0

[0;32m[VAIQ_NOTE]: =>Dumping 'ResidualBlock'' checking data...[0m

[0;32m[VAIQ_NOTE]: =>Finish dumping data.(quantize_result/deploy_check_data_int/ResidualBlock)[0m

[0;32m[VAIQ_NOTE]: =>Successfully convert 'ResidualBlock' to xmodel.(quantize_result/ResidualBlock_int.xmodel)[0m
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/quantizerimpl.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if inf.sum() > 0 or nan.sum() > 0:
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/fix_ops.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (isinstance(tensor, torch.Tensor) and
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/adaptive_avg_pool.py:41: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  input_size = [int(dim) for dim in input.shape[2:]]

[0;32m[VAIQ_NOTE]: ResidualBlock_int.pt is generated.(quantize_result/ResidualBlock_int.pt)[0m
Xmodel deployed
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input ResidualBlock::input_0
  warnings.warn(

[0;32m[VAIQ_NOTE]: ResidualBlock_int.onnx is generated.(quantize_result/ResidualBlock_int.onnx)[0m
-------- End of ResidualBlock-in_96x14x14-out_136-k_3-e_4-s_1-act_h_swish-use_se_True.pth test 
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: null
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B2304_0101000016010405
[UNILOG][INFO] Graph name: ResidualBlock, with op num: 55
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/ResidualBlock-in_96x14x14-out_136-k_3-e_4-s_1-act_h_swish-use_se_True.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 298385e35877ad737ab784e0f6033a72, and has been saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************



---------------------------Done Block 22-------------------------


-----------------------Strating Qt ResidualBlock-in_96 14 14-out_136-k_5-e_4-s_1-act_h_swish-use_se_True.pth -----------------------------

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_96x14x14-out_136-k_5-e_4-s_1-act_h_swish-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization calibration process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/16 [00:00<?, ?it/s]                                                  | 0/16 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–                                              | 1/16 [00:00<00:00, 5356.71it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 2/16 [00:00<00:00, 1137.75it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 3/16 [00:00<00:00, 1167.03it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 4/16 [00:00<00:00, 1295.74it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                  | 5/16 [00:00<00:00, 1309.98it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                               | 6/16 [00:00<00:00, 1478.52it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 7/16 [00:00<00:00, 1663.37it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1195, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 8/16 [00:00<00:00, 1782.44it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 9/16 [00:00<00:00, 1894.45it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 10/16 [00:00<00:00, 1997.29it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 11/16 [00:00<00:00, 2048.82it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 12/16 [00:00<00:00, 2026.80it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 13/16 [00:00<00:00, 1912.92it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 14/16 [00:00<00:00, 1915.64it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret, type = batch_norm]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 15/16 [00:00<00:00, 1989.90it/s, OpInfo: name = return_0, type = Return]                                                                        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 2093.55it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 13
  0%|          | 0/13 [00:00<?, ?it/s]/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/torchquantizer.py:223: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  bnfp[1] = stats.mode(data)[0][0]
  8%|â–Š         | 1/13 [00:00<00:05,  2.09it/s] 15%|â–ˆâ–Œ        | 2/13 [00:00<00:05,  2.14it/s] 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:01<00:04,  2.25it/s] 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:01<00:03,  2.30it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:02<00:03,  2.35it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:02<00:02,  2.40it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:03<00:02,  2.40it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:03<00:02,  2.23it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:03<00:01,  2.28it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:04<00:01,  2.33it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:04<00:00,  2.35it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:05<00:00,  2.39it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:05<00:00,  2.82it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:05<00:00,  2.42it/s]

[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quantized/quant_info.json)[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0
-------- End of ResidualBlock-in_96x14x14-out_136-k_5-e_4-s_1-act_h_swish-use_se_True.pth test 
Compilation

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_96x14x14-out_136-k_5-e_4-s_1-act_h_swish-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization test process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/16 [00:00<?, ?it/s]                                                  | 0/16 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–                                              | 1/16 [00:00<00:00, 5890.88it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 2/16 [00:00<00:00, 1883.39it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 3/16 [00:00<00:00, 1737.49it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 4/16 [00:00<00:00, 1660.78it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                  | 5/16 [00:00<00:00, 1417.86it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                               | 6/16 [00:00<00:00, 1419.87it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 7/16 [00:00<00:00, 1511.07it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1195, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 8/16 [00:00<00:00, 1591.24it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 9/16 [00:00<00:00, 1683.78it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 10/16 [00:00<00:00, 1782.08it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 11/16 [00:00<00:00, 1857.68it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 12/16 [00:00<00:00, 1919.52it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 13/16 [00:00<00:00, 1903.57it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 14/16 [00:00<00:00, 1966.06it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret, type = batch_norm]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 15/16 [00:00<00:00, 2016.04it/s, OpInfo: name = return_0, type = Return]                                                                        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 2118.54it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 1
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 24.07it/s]

[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0

[0;32m[VAIQ_NOTE]: =>Dumping 'ResidualBlock'' checking data...[0m

[0;32m[VAIQ_NOTE]: =>Finish dumping data.(quantize_result/deploy_check_data_int/ResidualBlock)[0m

[0;32m[VAIQ_NOTE]: =>Successfully convert 'ResidualBlock' to xmodel.(quantize_result/ResidualBlock_int.xmodel)[0m
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/quantizerimpl.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if inf.sum() > 0 or nan.sum() > 0:
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/fix_ops.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (isinstance(tensor, torch.Tensor) and
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/adaptive_avg_pool.py:41: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  input_size = [int(dim) for dim in input.shape[2:]]

[0;32m[VAIQ_NOTE]: ResidualBlock_int.pt is generated.(quantize_result/ResidualBlock_int.pt)[0m
Xmodel deployed
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input ResidualBlock::input_0
  warnings.warn(

[0;32m[VAIQ_NOTE]: ResidualBlock_int.onnx is generated.(quantize_result/ResidualBlock_int.onnx)[0m
-------- End of ResidualBlock-in_96x14x14-out_136-k_5-e_4-s_1-act_h_swish-use_se_True.pth test 
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: null
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B2304_0101000016010405
[UNILOG][INFO] Graph name: ResidualBlock, with op num: 55
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/ResidualBlock-in_96x14x14-out_136-k_5-e_4-s_1-act_h_swish-use_se_True.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is f7fff062ed29847de0a264e31a998c14, and has been saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************



---------------------------Done Block 23-------------------------


-----------------------Strating Qt ResidualBlock-in_96 14 14-out_136-k_7-e_4-s_1-act_h_swish-use_se_True.pth -----------------------------

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_96x14x14-out_136-k_7-e_4-s_1-act_h_swish-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization calibration process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/16 [00:00<?, ?it/s]                                                  | 0/16 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–                                              | 1/16 [00:00<00:00, 6069.90it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 2/16 [00:00<00:00, 1933.75it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 3/16 [00:00<00:00, 1702.23it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 4/16 [00:00<00:00, 1606.71it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                  | 5/16 [00:00<00:00, 1374.55it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                               | 6/16 [00:00<00:00, 1394.69it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 7/16 [00:00<00:00, 1505.80it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1195, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 8/16 [00:00<00:00, 1597.07it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 9/16 [00:00<00:00, 1698.63it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 10/16 [00:00<00:00, 1793.43it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 11/16 [00:00<00:00, 1874.51it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 12/16 [00:00<00:00, 1942.18it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 13/16 [00:00<00:00, 1942.57it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 14/16 [00:00<00:00, 2010.14it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret, type = batch_norm]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 15/16 [00:00<00:00, 2084.37it/s, OpInfo: name = return_0, type = Return]                                                                        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 2189.02it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 13
  0%|          | 0/13 [00:00<?, ?it/s]/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/torchquantizer.py:223: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  bnfp[1] = stats.mode(data)[0][0]
  8%|â–Š         | 1/13 [00:00<00:05,  2.05it/s] 15%|â–ˆâ–Œ        | 2/13 [00:00<00:05,  2.16it/s] 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:01<00:04,  2.23it/s] 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:01<00:03,  2.27it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:02<00:03,  2.25it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:02<00:03,  2.27it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:03<00:02,  2.30it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:03<00:02,  2.33it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:03<00:01,  2.38it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:04<00:01,  2.32it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:04<00:00,  2.34it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:05<00:00,  2.38it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:05<00:00,  2.76it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:05<00:00,  2.40it/s]

[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quantized/quant_info.json)[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0
-------- End of ResidualBlock-in_96x14x14-out_136-k_7-e_4-s_1-act_h_swish-use_se_True.pth test 
Compilation

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_96x14x14-out_136-k_7-e_4-s_1-act_h_swish-use_se_True.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization test process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/16 [00:00<?, ?it/s]                                                  | 0/16 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–                                              | 1/16 [00:00<00:00, 7724.32it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 2/16 [00:00<00:00, 2518.35it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 3/16 [00:00<00:00, 2663.05it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 4/16 [00:00<00:00, 2871.83it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                  | 5/16 [00:00<00:00, 2302.29it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                               | 6/16 [00:00<00:00, 2502.07it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 7/16 [00:00<00:00, 2751.14it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/AdaptiveAvgPool2d[avgpool]/1195, type = adaptive_avg_pool2d]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 8/16 [00:00<00:00, 2822.07it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc1]/ret.15, type = _convolution]                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 9/16 [00:00<00:00, 2917.21it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ReLU[activation]/ret.17, type = relu]   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 10/16 [00:00<00:00, 3010.12it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Conv2d[fc2]/ret.19, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 11/16 [00:00<00:00, 3058.29it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/Hardsigmoid[scale_activation]/ret.21, type = hardsigmoid]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 12/16 [00:00<00:00, 3108.81it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/SqueezeExcitation[se]/ret.23, type = mul]                                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 13/16 [00:00<00:00, 2983.80it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.25, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 14/16 [00:00<00:00, 3046.92it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret, type = batch_norm]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 15/16 [00:00<00:00, 3090.56it/s, OpInfo: name = return_0, type = Return]                                                                        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 3227.63it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 1
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 26.57it/s]

[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0

[0;32m[VAIQ_NOTE]: =>Dumping 'ResidualBlock'' checking data...[0m

[0;32m[VAIQ_NOTE]: =>Finish dumping data.(quantize_result/deploy_check_data_int/ResidualBlock)[0m

[0;32m[VAIQ_NOTE]: =>Successfully convert 'ResidualBlock' to xmodel.(quantize_result/ResidualBlock_int.xmodel)[0m
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/quantizerimpl.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if inf.sum() > 0 or nan.sum() > 0:
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/fix_ops.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (isinstance(tensor, torch.Tensor) and
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/adaptive_avg_pool.py:41: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  input_size = [int(dim) for dim in input.shape[2:]]

[0;32m[VAIQ_NOTE]: ResidualBlock_int.pt is generated.(quantize_result/ResidualBlock_int.pt)[0m
Xmodel deployed
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input ResidualBlock::input_0
  warnings.warn(

[0;32m[VAIQ_NOTE]: ResidualBlock_int.onnx is generated.(quantize_result/ResidualBlock_int.onnx)[0m
-------- End of ResidualBlock-in_96x14x14-out_136-k_7-e_4-s_1-act_h_swish-use_se_True.pth test 
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: null
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B2304_0101000016010405
[UNILOG][INFO] Graph name: ResidualBlock, with op num: 55
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/ResidualBlock-in_96x14x14-out_136-k_7-e_4-s_1-act_h_swish-use_se_True.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 2c456e37b5cf9c48d4832c5327fe6ee7, and has been saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************



---------------------------Done Block 24-------------------------


-----------------------Strating Qt ResidualBlock-in_96 14 14-out_96-k_3-e_6-s_1-act_h_swish-use_se_False.pth -----------------------------

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_96x14x14-out_96-k_3-e_6-s_1-act_h_swish-use_se_False.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization calibration process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/11 [00:00<?, ?it/s]                                                  | 0/11 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                             | 1/11 [00:00<00:00, 6078.70it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                         | 2/11 [00:00<00:00, 1894.02it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                    | 3/11 [00:00<00:00, 1660.23it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 4/11 [00:00<00:00, 1696.72it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                           | 5/11 [00:00<00:00, 1884.40it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                      | 6/11 [00:00<00:00, 2074.68it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 7/11 [00:00<00:00, 2302.21it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.15, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž             | 8/11 [00:00<00:00, 2441.92it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.17, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 9/11 [00:00<00:00, 2556.12it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 10/11 [00:00<00:00, 2322.17it/s, OpInfo: name = return_0, type = Return]     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 2392.89it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 13
  0%|          | 0/13 [00:00<?, ?it/s]/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/torchquantizer.py:223: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  bnfp[1] = stats.mode(data)[0][0]
  8%|â–Š         | 1/13 [00:00<00:06,  1.86it/s] 15%|â–ˆâ–Œ        | 2/13 [00:01<00:05,  1.86it/s] 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:01<00:05,  1.90it/s] 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:02<00:04,  1.94it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:02<00:04,  1.99it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:03<00:03,  2.00it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:04<00:04,  1.21it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:05<00:03,  1.32it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:05<00:02,  1.48it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:06<00:01,  1.62it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:06<00:01,  1.74it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:07<00:00,  1.85it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:07<00:00,  2.16it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:07<00:00,  1.76it/s]

[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quantized/quant_info.json)[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0
-------- End of ResidualBlock-in_96x14x14-out_96-k_3-e_6-s_1-act_h_swish-use_se_False.pth test 
Compilation

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_96x14x14-out_96-k_3-e_6-s_1-act_h_swish-use_se_False.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization test process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/11 [00:00<?, ?it/s]                                                  | 0/11 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                             | 1/11 [00:00<00:00, 9986.44it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                         | 2/11 [00:00<00:00, 3400.33it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                    | 3/11 [00:00<00:00, 3306.07it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 4/11 [00:00<00:00, 3344.07it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                           | 5/11 [00:00<00:00, 3401.71it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                      | 6/11 [00:00<00:00, 3538.00it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 7/11 [00:00<00:00, 3799.19it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.15, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž             | 8/11 [00:00<00:00, 3854.17it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.17, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 9/11 [00:00<00:00, 3912.60it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 10/11 [00:00<00:00, 3638.05it/s, OpInfo: name = return_0, type = Return]     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 3866.04it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 1
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 16.40it/s]

[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0

[0;32m[VAIQ_NOTE]: =>Dumping 'ResidualBlock'' checking data...[0m

[0;32m[VAIQ_NOTE]: =>Finish dumping data.(quantize_result/deploy_check_data_int/ResidualBlock)[0m

[0;32m[VAIQ_NOTE]: =>Successfully convert 'ResidualBlock' to xmodel.(quantize_result/ResidualBlock_int.xmodel)[0m
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/quantizerimpl.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if inf.sum() > 0 or nan.sum() > 0:
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/fix_ops.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (isinstance(tensor, torch.Tensor) and

[0;32m[VAIQ_NOTE]: ResidualBlock_int.pt is generated.(quantize_result/ResidualBlock_int.pt)[0m
Xmodel deployed
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input ResidualBlock::input_0
  warnings.warn(

[0;32m[VAIQ_NOTE]: ResidualBlock_int.onnx is generated.(quantize_result/ResidualBlock_int.onnx)[0m
-------- End of ResidualBlock-in_96x14x14-out_96-k_3-e_6-s_1-act_h_swish-use_se_False.pth test 
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: null
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B2304_0101000016010405
[UNILOG][INFO] Graph name: ResidualBlock, with op num: 34
[UNILOG][INFO] Begin to compile...
module_infer = 0, counter_m[module_infer] = 50, counter_p[module_infer] = 50
module_idx = 0, counter_m[module_idx] = 50, counter_p[module_idx] = 50
module_idx = 1, counter_m[module_idx] = 0, counter_p[module_idx] = 0
module_idx = 2, counter_m[module_idx] = 48, counter_p[module_idx] = 48
module_idx = 3, counter_m[module_idx] = 2, counter_p[module_idx] = 2
[0;33m[UNILOG][WARNING] subgraph_ResidualBlock__ResidualBlock_MBConvLayer_conv__Sequential_depth_conv__Conv2d_conv__ret_9 switch to no prefetch mode because of PM failure: .
[m[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/ResidualBlock-in_96x14x14-out_96-k_3-e_6-s_1-act_h_swish-use_se_False.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 1f11a58f983123cb4c63e76c6b859cf9, and has been saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************



---------------------------Done Block 25-------------------------


-----------------------Strating Qt ResidualBlock-in_96 14 14-out_96-k_5-e_6-s_1-act_h_swish-use_se_False.pth -----------------------------

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_96x14x14-out_96-k_5-e_6-s_1-act_h_swish-use_se_False.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization calibration process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/11 [00:00<?, ?it/s]                                                  | 0/11 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                             | 1/11 [00:00<00:00, 9776.93it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                         | 2/11 [00:00<00:00, 3211.57it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                    | 3/11 [00:00<00:00, 3045.97it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 4/11 [00:00<00:00, 3115.55it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                           | 5/11 [00:00<00:00, 2928.17it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                      | 6/11 [00:00<00:00, 2667.00it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 7/11 [00:00<00:00, 2849.67it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.15, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž             | 8/11 [00:00<00:00, 2927.45it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.17, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 9/11 [00:00<00:00, 2964.87it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 10/11 [00:00<00:00, 2543.70it/s, OpInfo: name = return_0, type = Return]     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 2579.38it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 13
  0%|          | 0/13 [00:00<?, ?it/s]/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/torchquantizer.py:223: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  bnfp[1] = stats.mode(data)[0][0]
  8%|â–Š         | 1/13 [00:00<00:06,  1.86it/s] 15%|â–ˆâ–Œ        | 2/13 [00:01<00:05,  1.96it/s] 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:01<00:04,  2.01it/s] 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:02<00:04,  2.02it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:02<00:04,  1.89it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:03<00:03,  1.93it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:03<00:03,  1.94it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:04<00:02,  1.97it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:04<00:01,  2.00it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:05<00:01,  2.01it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:05<00:01,  1.99it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:06<00:00,  2.02it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:06<00:00,  2.35it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:06<00:00,  2.05it/s]

[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quantized/quant_info.json)[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0
-------- End of ResidualBlock-in_96x14x14-out_96-k_5-e_6-s_1-act_h_swish-use_se_False.pth test 
Compilation

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_96x14x14-out_96-k_5-e_6-s_1-act_h_swish-use_se_False.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization test process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/11 [00:00<?, ?it/s]                                                  | 0/11 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                             | 1/11 [00:00<00:00, 8050.49it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                         | 2/11 [00:00<00:00, 2657.99it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                    | 3/11 [00:00<00:00, 2814.97it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 4/11 [00:00<00:00, 3029.47it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                           | 5/11 [00:00<00:00, 3201.27it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                      | 6/11 [00:00<00:00, 3360.82it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 7/11 [00:00<00:00, 3563.12it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.15, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž             | 8/11 [00:00<00:00, 3554.12it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.17, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 9/11 [00:00<00:00, 3510.20it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 10/11 [00:00<00:00, 2935.13it/s, OpInfo: name = return_0, type = Return]     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 2978.33it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 1
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 22.79it/s]

[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0

[0;32m[VAIQ_NOTE]: =>Dumping 'ResidualBlock'' checking data...[0m

[0;32m[VAIQ_NOTE]: =>Finish dumping data.(quantize_result/deploy_check_data_int/ResidualBlock)[0m

[0;32m[VAIQ_NOTE]: =>Successfully convert 'ResidualBlock' to xmodel.(quantize_result/ResidualBlock_int.xmodel)[0m
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/quantizerimpl.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if inf.sum() > 0 or nan.sum() > 0:
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/fix_ops.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (isinstance(tensor, torch.Tensor) and

[0;32m[VAIQ_NOTE]: ResidualBlock_int.pt is generated.(quantize_result/ResidualBlock_int.pt)[0m
Xmodel deployed
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input ResidualBlock::input_0
  warnings.warn(

[0;32m[VAIQ_NOTE]: ResidualBlock_int.onnx is generated.(quantize_result/ResidualBlock_int.onnx)[0m
-------- End of ResidualBlock-in_96x14x14-out_96-k_5-e_6-s_1-act_h_swish-use_se_False.pth test 
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: null
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B2304_0101000016010405
[UNILOG][INFO] Graph name: ResidualBlock, with op num: 34
[UNILOG][INFO] Begin to compile...
module_infer = 0, counter_m[module_infer] = 50, counter_p[module_infer] = 50
module_idx = 0, counter_m[module_idx] = 50, counter_p[module_idx] = 50
module_idx = 1, counter_m[module_idx] = 0, counter_p[module_idx] = 0
module_idx = 2, counter_m[module_idx] = 48, counter_p[module_idx] = 48
module_idx = 3, counter_m[module_idx] = 2, counter_p[module_idx] = 2
[0;33m[UNILOG][WARNING] subgraph_ResidualBlock__ResidualBlock_MBConvLayer_conv__Sequential_depth_conv__Conv2d_conv__ret_9 switch to no prefetch mode because of PM failure: .
[m[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/ResidualBlock-in_96x14x14-out_96-k_5-e_6-s_1-act_h_swish-use_se_False.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 4ceec0b2154799d05a3a6b906585d641, and has been saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************



---------------------------Done Block 26-------------------------


-----------------------Strating Qt ResidualBlock-in_96 14 14-out_96-k_7-e_6-s_1-act_h_swish-use_se_False.pth -----------------------------

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_96x14x14-out_96-k_7-e_6-s_1-act_h_swish-use_se_False.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization calibration process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/11 [00:00<?, ?it/s]                                                  | 0/11 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                             | 1/11 [00:00<00:00, 9467.95it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                         | 2/11 [00:00<00:00, 3258.98it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                    | 3/11 [00:00<00:00, 3194.44it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 4/11 [00:00<00:00, 3110.92it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                           | 5/11 [00:00<00:00, 2819.51it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                      | 6/11 [00:00<00:00, 2738.99it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 7/11 [00:00<00:00, 2904.07it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.15, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž             | 8/11 [00:00<00:00, 2958.42it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.17, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 9/11 [00:00<00:00, 3010.27it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 10/11 [00:00<00:00, 2656.81it/s, OpInfo: name = return_0, type = Return]     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 2714.44it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 13
  0%|          | 0/13 [00:00<?, ?it/s]/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/torchquantizer.py:223: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.
  bnfp[1] = stats.mode(data)[0][0]
  8%|â–Š         | 1/13 [00:00<00:06,  1.86it/s] 15%|â–ˆâ–Œ        | 2/13 [00:01<00:05,  1.89it/s] 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:01<00:05,  1.95it/s] 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:02<00:04,  1.99it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:02<00:04,  1.99it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:03<00:03,  1.99it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:03<00:03,  1.99it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:04<00:02,  1.99it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:04<00:02,  2.00it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:05<00:01,  2.02it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:05<00:00,  2.02it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:06<00:00,  2.01it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:06<00:00,  2.31it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:06<00:00,  2.06it/s]

[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quantized/quant_info.json)[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0
-------- End of ResidualBlock-in_96x14x14-out_96-k_7-e_6-s_1-act_h_swish-use_se_False.pth test 
Compilation

[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...[0m

[0;32m[VAIQ_NOTE]: OS and CPU information:
               system --- Linux
                 node --- 4e796a113d1a
              release --- 6.2.0-39-generic
              version --- #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2
              machine --- x86_64
            processor --- x86_64[0m
-------- Start ResidualBlock-in_96x14x14-out_96-k_7-e_6-s_1-act_h_swish-use_se_False.pth test 
=== Load pretrained model ===

[0;32m[VAIQ_NOTE]: Tools version information:
                  GCC --- GCC 7.5.0
               python --- 3.8.6
              pytorch --- 1.13.1
        vai_q_pytorch --- 3.5.0+60df3f1+torch1.13.1[0m

[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration[0m

[0;32m[VAIQ_NOTE]: Quantization test process start up...[0m

[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.[0m

[0;32m[VAIQ_NOTE]: =>Parsing ResidualBlock...[0m

[0;32m[VAIQ_NOTE]: Start to trace and freeze model...[0m

[0;32m[VAIQ_NOTE]: The input model nndct_st_ResidualBlock_ed is torch.nn.Module.[0m

[0;32m[VAIQ_NOTE]: Finish tracing.[0m

[0;32m[VAIQ_NOTE]: Processing ops...[0m
                                                  | 0/11 [00:00<?, ?it/s]                                                  | 0/11 [00:00<?, ?it/s, OpInfo: name = input_0, type = Param]â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                             | 1/11 [00:00<00:00, 7319.90it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Conv2d[conv]/ret.3, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                         | 2/11 [00:00<00:00, 2407.75it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/BatchNorm2d[bn]/ret.5, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                    | 3/11 [00:00<00:00, 2588.54it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[inverted_bottleneck]/Hardswish[act]/ret.7, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 4/11 [00:00<00:00, 2755.78it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Conv2d[conv]/ret.9, type = _convolution]        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                           | 5/11 [00:00<00:00, 2954.98it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/BatchNorm2d[bn]/ret.11, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                      | 6/11 [00:00<00:00, 3134.76it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[depth_conv]/Hardswish[act]/ret.13, type = hardswish]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 7/11 [00:00<00:00, 3413.97it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/Conv2d[conv]/ret.15, type = _convolution]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž             | 8/11 [00:00<00:00, 3515.39it/s, OpInfo: name = ResidualBlock/MBConvLayer[conv]/Sequential[point_linear]/BatchNorm2d[bn]/ret.17, type = batch_norm]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 9/11 [00:00<00:00, 3618.90it/s, OpInfo: name = ResidualBlock/ret, type = add]                                                                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 10/11 [00:00<00:00, 3298.45it/s, OpInfo: name = return_0, type = Return]     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 3499.50it/s, OpInfo: name = return_0, type = Return]

[0;32m[VAIQ_NOTE]: =>Doing weights equalization...[0m

[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quantized/ResidualBlock.py)[0m

[0;32m[VAIQ_NOTE]: =>Get module with quantization.[0m
Random Dataset Generated !, 1
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 25.02it/s]

[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...[0m
Evaluation Skipped
Dummy Values 

loss: 0.98
top-1 / top-5 accuracy: 50.0 / 50.0

[0;32m[VAIQ_NOTE]: =>Dumping 'ResidualBlock'' checking data...[0m

[0;32m[VAIQ_NOTE]: =>Finish dumping data.(quantize_result/deploy_check_data_int/ResidualBlock)[0m

[0;32m[VAIQ_NOTE]: =>Successfully convert 'ResidualBlock' to xmodel.(quantize_result/ResidualBlock_int.xmodel)[0m
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/prim_ops.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not (list(self.node.out_tensors[0].shape[1:]) == list(input.size())[1:]):
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/quantization/quantizerimpl.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if inf.sum() > 0 or nan.sum() > 0:
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/nn/modules/fix_ops.py:67: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (isinstance(tensor, torch.Tensor) and

[0;32m[VAIQ_NOTE]: ResidualBlock_int.pt is generated.(quantize_result/ResidualBlock_int.pt)[0m
Xmodel deployed
/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input ResidualBlock::input_0
  warnings.warn(

[0;32m[VAIQ_NOTE]: ResidualBlock_int.onnx is generated.(quantize_result/ResidualBlock_int.onnx)[0m
-------- End of ResidualBlock-in_96x14x14-out_96-k_7-e_6-s_1-act_h_swish-use_se_False.pth test 
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: null
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B2304_0101000016010405
[UNILOG][INFO] Graph name: ResidualBlock, with op num: 34
[UNILOG][INFO] Begin to compile...
module_infer = 0, counter_m[module_infer] = 50, counter_p[module_infer] = 50
module_idx = 0, counter_m[module_idx] = 50, counter_p[module_idx] = 50
module_idx = 1, counter_m[module_idx] = 0, counter_p[module_idx] = 0
module_idx = 2, counter_m[module_idx] = 48, counter_p[module_idx] = 48
module_idx = 3, counter_m[module_idx] = 2, counter_p[module_idx] = 2
[0;33m[UNILOG][WARNING] subgraph_ResidualBlock__ResidualBlock_MBConvLayer_conv__Sequential_depth_conv__Conv2d_conv__ret_9 switch to no prefetch mode because of PM failure: .
[m[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/ResidualBlock-in_96x14x14-out_96-k_7-e_6-s_1-act_h_swish-use_se_False.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 077081f9c7a172200f7f4346b1a29c01, and has been saved to "/workspace/Quantizing-Efficientnetv2-using-Vitis-AI-Pytorch/blocks_quantization_vitis_ai_3.0/./compiled/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************



---------------------------Done Block 27-------------------------


